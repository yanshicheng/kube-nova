package operator

import (
	"context"
	"fmt"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/yanshicheng/kube-nova/common/prometheusmanager/types"
	"github.com/zeromicro/go-zero/core/logx"
)

type ClusterOperator struct {
	log logx.Logger
	ctx context.Context
	*BaseOperator
}

func NewClusterOperator(ctx context.Context, base *BaseOperator) types.ClusterOperator {
	return &ClusterOperator{
		log:          logx.WithContext(ctx),
		ctx:          ctx,
		BaseOperator: base,
	}
}

// ==================== å¹¶å‘æŸ¥è¯¢è¾…åŠ©æ–¹æ³• ====================

type clusterQueryTask struct {
	name  string
	query string
	f     func([]types.InstantQueryResult) error
}

type clusterRangeTask struct {
	name  string
	query string
	f     func([]types.RangeQueryResult) error
}

func (c *ClusterOperator) executeParallelQueries(tasks []clusterQueryTask) {
	var wg sync.WaitGroup
	for _, task := range tasks {
		wg.Add(1)
		go func(t clusterQueryTask) {
			defer wg.Done()
			results, err := c.query(t.query, nil)
			if err != nil {
				c.log.Errorf("æŸ¥è¯¢å¤±è´¥ [%s]: %v", t.name, err)
				return
			}
			if err := t.f(results); err != nil {
				c.log.Errorf("å¤„ç†ç»“æžœå¤±è´¥ [%s]: %v", t.name, err)
			}
		}(task)
	}
	wg.Wait()
}

func (c *ClusterOperator) executeParallelRangeQueries(start, end time.Time, step string, tasks []clusterRangeTask) {
	var wg sync.WaitGroup
	for _, task := range tasks {
		wg.Add(1)
		go func(t clusterRangeTask) {
			defer wg.Done()
			results, err := c.queryRange(t.query, start, end, step)
			if err != nil {
				c.log.Errorf("èŒƒå›´æŸ¥è¯¢å¤±è´¥ [%s]: %v", t.name, err)
				return
			}
			if err := t.f(results); err != nil {
				c.log.Errorf("å¤„ç†èŒƒå›´ç»“æžœå¤±è´¥ [%s]: %v", t.name, err)
			}
		}(task)
	}
	wg.Wait()
}

// ==================== ç»¼åˆæŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================

func (c *ClusterOperator) GetClusterOverview(timeRange *types.TimeRange) (*types.ClusterOverview, error) {
	c.log.Infof("ðŸ“Š æŸ¥è¯¢é›†ç¾¤æ¦‚è§ˆ")

	overview := &types.ClusterOverview{
		ClusterName: "kubernetes-cluster",
		Timestamp:   time.Now(),
	}

	var wg sync.WaitGroup
	wg.Add(6)

	go func() {
		defer wg.Done()
		if resources, err := c.GetClusterResources(timeRange); err == nil {
			overview.Resources = *resources
		} else {
			c.log.Errorf("èŽ·å–èµ„æºæŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	go func() {
		defer wg.Done()
		if nodes, err := c.GetClusterNodes(timeRange); err == nil {
			overview.Nodes = *nodes
		} else {
			c.log.Errorf("èŽ·å–èŠ‚ç‚¹æŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	go func() {
		defer wg.Done()
		if controlPlane, err := c.getControlPlaneMetrics(timeRange); err == nil {
			overview.ControlPlane = *controlPlane
		} else {
			c.log.Errorf("èŽ·å–æŽ§åˆ¶å¹³é¢æŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	go func() {
		defer wg.Done()
		if workloads, err := c.GetClusterWorkloads(timeRange); err == nil {
			overview.Workloads = *workloads
		} else {
			c.log.Errorf("èŽ·å–å·¥ä½œè´Ÿè½½æŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	go func() {
		defer wg.Done()
		if network, err := c.GetClusterNetwork(timeRange); err == nil {
			overview.Network = *network
		} else {
			c.log.Errorf("èŽ·å–ç½‘ç»œæŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	go func() {
		defer wg.Done()
		if storage, err := c.GetClusterStorage(); err == nil {
			overview.Storage = *storage
		} else {
			c.log.Errorf("èŽ·å–å­˜å‚¨æŒ‡æ ‡å¤±è´¥: %v", err)
		}
	}()

	wg.Wait()

	c.log.Infof("âœ… é›†ç¾¤æ¦‚è§ˆæŸ¥è¯¢å®Œæˆ")
	return overview, nil
}

// ==================== èµ„æºæŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================

func (c *ClusterOperator) GetClusterResources(timeRange *types.TimeRange) (*types.ClusterResourceMetrics, error) {
	resources := &types.ClusterResourceMetrics{}

	var wg sync.WaitGroup
	wg.Add(2)

	go func() {
		defer wg.Done()
		if cpu, err := c.GetClusterCPUMetrics(timeRange); err == nil {
			resources.CPU = *cpu
		}
	}()

	go func() {
		defer wg.Done()
		if memory, err := c.GetClusterMemoryMetrics(timeRange); err == nil {
			resources.Memory = *memory
		}
	}()

	wg.Wait()

	// åˆå¹¶æŸ¥è¯¢ Pods å’Œ Storage
	tasks := []clusterQueryTask{
		{
			name:  "pods_running",
			query: `sum(kube_pod_status_phase{phase="Running"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					resources.Pods.Running = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pods_capacity",
			query: `sum(kube_node_status_capacity{resource="pods"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					resources.Pods.Capacity = int64(results[0].Value)
					if resources.Pods.Capacity > 0 {
						resources.Pods.UsagePercent = (float64(resources.Pods.Running) / float64(resources.Pods.Capacity)) * 100
					}
				}
				return nil
			},
		},
		{
			name:  "pv_capacity",
			query: `sum(kube_persistentvolume_capacity_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					resources.Storage.TotalCapacityBytes = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pvc_requests",
			query: `sum(kube_persistentvolumeclaim_resource_requests_storage_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					resources.Storage.AllocatedBytes = int64(results[0].Value)
					if resources.Storage.TotalCapacityBytes > 0 {
						resources.Storage.AllocationPercent = (float64(resources.Storage.AllocatedBytes) / float64(resources.Storage.TotalCapacityBytes)) * 100
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return resources, nil
}

func (c *ClusterOperator) GetClusterCPUMetrics(timeRange *types.TimeRange) (*types.ClusterResourceSummary, error) {
	cpu := &types.ClusterResourceSummary{Trend: []types.ClusterResourceDataPoint{}}
	window := c.calculateRateWindow(timeRange)

	// åˆå¹¶æŸ¥è¯¢æ‰€æœ‰ CPU æŒ‡æ ‡
	tasks := []clusterQueryTask{
		{
			name:  "cpu_capacity",
			query: `sum(kube_node_status_capacity{resource="cpu"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.Capacity = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "cpu_allocatable",
			query: `sum(kube_node_status_allocatable{resource="cpu"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.Allocatable = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "cpu_requests",
			query: `sum(kube_pod_container_resource_requests{resource="cpu"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.RequestsAllocated = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "cpu_limits",
			query: `sum(kube_pod_container_resource_limits{resource="cpu"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.LimitsAllocated = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "cpu_usage",
			query: fmt.Sprintf(`sum(rate(container_cpu_usage_seconds_total{container!=""}[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.Usage = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "cpu_no_requests",
			query: `count(kube_pod_container_resource_requests{resource="cpu"} == 0)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.NoRequestsCount = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "cpu_no_limits",
			query: `count(kube_pod_container_resource_limits{resource="cpu"} == 0)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cpu.NoLimitsCount = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// è®¡ç®—ç™¾åˆ†æ¯”
	if cpu.Allocatable > 0 {
		cpu.RequestsPercent = (cpu.RequestsAllocated / cpu.Allocatable) * 100
		cpu.UsagePercent = (cpu.Usage / cpu.Allocatable) * 100
	}

	// è¶‹åŠ¿æ•°æ®
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		trendQuery := fmt.Sprintf(`sum(rate(container_cpu_usage_seconds_total{container!=""}[%s]))`, window)
		if trendResult, err := c.queryRange(trendQuery, timeRange.Start, timeRange.End, step); err == nil && len(trendResult) > 0 {
			cpu.Trend = make([]types.ClusterResourceDataPoint, len(trendResult[0].Values))
			for i, v := range trendResult[0].Values {
				usagePercent := 0.0
				requestsPercent := 0.0
				if cpu.Allocatable > 0 {
					usagePercent = (v.Value / cpu.Allocatable) * 100
					requestsPercent = (cpu.RequestsAllocated / cpu.Allocatable) * 100
				}
				cpu.Trend[i] = types.ClusterResourceDataPoint{
					Timestamp:       v.Timestamp,
					Usage:           v.Value,
					UsagePercent:    usagePercent,
					RequestsPercent: requestsPercent,
				}
			}
		}
	}

	return cpu, nil
}

func (c *ClusterOperator) GetClusterMemoryMetrics(timeRange *types.TimeRange) (*types.ClusterResourceSummary, error) {
	memory := &types.ClusterResourceSummary{Trend: []types.ClusterResourceDataPoint{}}

	// åˆå¹¶æŸ¥è¯¢æ‰€æœ‰å†…å­˜æŒ‡æ ‡
	tasks := []clusterQueryTask{
		{
			name:  "memory_capacity",
			query: `sum(kube_node_status_capacity{resource="memory"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					memory.Capacity = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "memory_allocatable",
			query: `sum(kube_node_status_allocatable{resource="memory"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					memory.Allocatable = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "memory_requests",
			query: `sum(kube_pod_container_resource_requests{resource="memory"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					memory.RequestsAllocated = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "memory_limits",
			query: `sum(kube_pod_container_resource_limits{resource="memory"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					memory.LimitsAllocated = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "memory_usage",
			query: `sum(container_memory_usage_bytes{container!=""})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					memory.Usage = results[0].Value
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// è®¡ç®—ç™¾åˆ†æ¯”
	if memory.Allocatable > 0 {
		memory.RequestsPercent = (memory.RequestsAllocated / memory.Allocatable) * 100
		memory.UsagePercent = (memory.Usage / memory.Allocatable) * 100
	}

	// è¶‹åŠ¿æ•°æ®
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		trendQuery := `sum(container_memory_usage_bytes{container!=""})`
		if trendResult, err := c.queryRange(trendQuery, timeRange.Start, timeRange.End, step); err == nil && len(trendResult) > 0 {
			memory.Trend = make([]types.ClusterResourceDataPoint, len(trendResult[0].Values))
			for i, v := range trendResult[0].Values {
				usagePercent := 0.0
				requestsPercent := 0.0
				if memory.Allocatable > 0 {
					usagePercent = (v.Value / memory.Allocatable) * 100
					requestsPercent = (memory.RequestsAllocated / memory.Allocatable) * 100
				}
				memory.Trend[i] = types.ClusterResourceDataPoint{
					Timestamp:       v.Timestamp,
					Usage:           v.Value,
					UsagePercent:    usagePercent,
					RequestsPercent: requestsPercent,
				}
			}
		}
	}

	return memory, nil
}

// ==================== èŠ‚ç‚¹æŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================

func (c *ClusterOperator) GetClusterNodes(timeRange *types.TimeRange) (*types.ClusterNodeMetrics, error) {
	nodes := &types.ClusterNodeMetrics{
		NodeList: []types.NodeBriefStatus{},
		Trend:    []types.ClusterNodeDataPoint{},
	}

	window := c.calculateRateWindow(timeRange)

	// åˆå¹¶æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹ç»Ÿè®¡
	tasks := []clusterQueryTask{
		{
			name:  "nodes_total",
			query: `count(kube_node_info)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "nodes_ready",
			query: `sum(kube_node_status_condition{condition="Ready",status="true"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.Ready = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "nodes_not_ready",
			query: `sum(kube_node_status_condition{condition="Ready",status="false"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.NotReady = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "nodes_unknown",
			query: `sum(kube_node_status_condition{condition="Ready",status="unknown"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.Unknown = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "memory_pressure",
			query: `sum(kube_node_status_condition{condition="MemoryPressure",status="true"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.MemoryPressure = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "disk_pressure",
			query: `sum(kube_node_status_condition{condition="DiskPressure",status="true"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.DiskPressure = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pid_pressure",
			query: `sum(kube_node_status_condition{condition="PIDPressure",status="true"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.PIDPressure = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "avg_cpu",
			query: fmt.Sprintf(`avg(rate(node_cpu_seconds_total{mode!="idle"}[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.AvgCPUUsage = results[0].Value * 100
				}
				return nil
			},
		},
		{
			name:  "avg_memory",
			query: `avg((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.AvgMemoryUsage = results[0].Value * 100
				}
				return nil
			},
		},
		{
			name:  "high_load",
			query: fmt.Sprintf(`count((rate(node_cpu_seconds_total{mode!="idle"}[%s]) > 0.8) OR ((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.8))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					nodes.HighLoadNodes = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// å¹¶å‘èŽ·å–èŠ‚ç‚¹åˆ—è¡¨
	go func() {
		if nodeList, err := c.getDetailedNodeListOptimized(timeRange); err == nil && len(nodeList) > 0 {
			nodes.NodeList = nodeList
		} else {
			c.log.Errorf("èŽ·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: %v", err)
		}
	}()

	// è¶‹åŠ¿æ•°æ®
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		var mu sync.Mutex
		rangeTasks := []clusterRangeTask{
			{
				name:  "ready_trend",
				query: `sum(kube_node_status_condition{condition="Ready",status="true"})`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()
						nodes.Trend = make([]types.ClusterNodeDataPoint, len(results[0].Values))
						for i, v := range results[0].Values {
							nodes.Trend[i] = types.ClusterNodeDataPoint{
								Timestamp:  v.Timestamp,
								ReadyNodes: int64(v.Value),
							}
						}
					}
					return nil
				},
			},
			{
				name:  "not_ready_trend",
				query: `sum(kube_node_status_condition{condition="Ready",status="false"})`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(nodes.Trend) {
								nodes.Trend[i].NotReadyNodes = int64(v.Value)
							}
						}
					}
					return nil
				},
			},
			{
				name:  "avg_cpu_trend",
				query: fmt.Sprintf(`avg(rate(node_cpu_seconds_total{mode!="idle"}[%s])) * 100`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(nodes.Trend) {
								nodes.Trend[i].AvgCPUUsage = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "avg_memory_trend",
				query: `avg((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * 100`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(nodes.Trend) {
								nodes.Trend[i].AvgMemoryUsage = v.Value
							}
						}
					}
					return nil
				},
			},
		}

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
	}

	return nodes, nil
}

func (c *ClusterOperator) getDetailedNodeListOptimized(timeRange *types.TimeRange) ([]types.NodeBriefStatus, error) {
	window := c.calculateRateWindow(timeRange)

	// 1. èŽ·å–æ‰€æœ‰èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
	nodeQuery := `kube_node_info`
	nodeResults, err := c.query(nodeQuery, nil)
	if err != nil {
		return nil, fmt.Errorf("æŸ¥è¯¢èŠ‚ç‚¹ä¿¡æ¯å¤±è´¥: %w", err)
	}

	if len(nodeResults) == 0 {
		return []types.NodeBriefStatus{}, nil
	}

	// åˆ›å»ºèŠ‚ç‚¹æ˜ å°„
	nodeMap := make(map[string]*types.NodeBriefStatus)
	for _, result := range nodeResults {
		if nodeName, ok := result.Metric["node"]; ok {
			nodeMap[nodeName] = &types.NodeBriefStatus{
				NodeName: nodeName,
			}
		}
	}

	// 2. å¹¶å‘æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹æŒ‡æ ‡
	var wg sync.WaitGroup
	wg.Add(5)

	go func() {
		defer wg.Done()
		readyQuery := `kube_node_status_condition{condition="Ready",status="true"}`
		if readyResults, err := c.query(readyQuery, nil); err == nil {
			for _, r := range readyResults {
				if nodeName, ok := r.Metric["node"]; ok {
					if node, exists := nodeMap[nodeName]; exists {
						node.Ready = r.Value > 0
					}
				}
			}
		}
	}()

	go func() {
		defer wg.Done()
		memPressureQuery := `kube_node_status_condition{condition="MemoryPressure",status="true"}`
		if memPressureResults, err := c.query(memPressureQuery, nil); err == nil {
			for _, r := range memPressureResults {
				if nodeName, ok := r.Metric["node"]; ok {
					if node, exists := nodeMap[nodeName]; exists {
						node.MemoryPressure = r.Value > 0
					}
				}
			}
		}
	}()

	go func() {
		defer wg.Done()
		diskPressureQuery := `kube_node_status_condition{condition="DiskPressure",status="true"}`
		if diskPressureResults, err := c.query(diskPressureQuery, nil); err == nil {
			for _, r := range diskPressureResults {
				if nodeName, ok := r.Metric["node"]; ok {
					if node, exists := nodeMap[nodeName]; exists {
						node.DiskPressure = r.Value > 0
					}
				}
			}
		}
	}()

	go func() {
		defer wg.Done()
		cpuQuery := fmt.Sprintf(`(1 - avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[%s]))) * 100`, window)
		if cpuResults, err := c.query(cpuQuery, nil); err == nil {
			for _, r := range cpuResults {
				if instance, ok := r.Metric["instance"]; ok {
					instanceParts := strings.Split(instance, ":")
					instanceHost := instanceParts[0]
					for nodeName, node := range nodeMap {
						if strings.Contains(instance, nodeName) || strings.Contains(instanceHost, nodeName) || strings.HasPrefix(nodeName, instanceHost) {
							node.CPUUsage = r.Value
							break
						}
					}
				}
			}
		}
	}()

	go func() {
		defer wg.Done()
		memQuery := `((node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) * 100`
		if memResults, err := c.query(memQuery, nil); err == nil {
			for _, r := range memResults {
				if instance, ok := r.Metric["instance"]; ok {
					instanceParts := strings.Split(instance, ":")
					instanceHost := instanceParts[0]
					for nodeName, node := range nodeMap {
						if strings.Contains(instance, nodeName) || strings.Contains(instanceHost, nodeName) || strings.HasPrefix(nodeName, instanceHost) {
							node.MemoryUsage = r.Value
							break
						}
					}
				}
			}
		}
	}()

	wg.Wait()

	// æ‰¹é‡æŸ¥è¯¢ Pod æ•°é‡
	podQuery := `count by (node) (kube_pod_info)`
	if podResults, err := c.query(podQuery, nil); err == nil {
		for _, r := range podResults {
			if nodeName, ok := r.Metric["node"]; ok {
				if node, exists := nodeMap[nodeName]; exists {
					node.PodsCount = int64(r.Value)
				}
			}
		}
	}

	// è½¬æ¢ä¸ºåˆ—è¡¨
	nodeList := make([]types.NodeBriefStatus, 0, len(nodeMap))
	for _, node := range nodeMap {
		nodeList = append(nodeList, *node)
	}

	return nodeList, nil
}

// ==================== æŽ§åˆ¶å¹³é¢æŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================

func (c *ClusterOperator) getControlPlaneMetrics(timeRange *types.TimeRange) (*types.ClusterControlPlaneMetrics, error) {
	controlPlane := &types.ClusterControlPlaneMetrics{}

	var wg sync.WaitGroup
	wg.Add(4)

	go func() {
		defer wg.Done()
		if apiServer, err := c.GetAPIServerMetrics(timeRange); err == nil {
			controlPlane.APIServer = *apiServer
		}
	}()

	go func() {
		defer wg.Done()
		if etcd, err := c.GetEtcdMetrics(timeRange); err == nil {
			controlPlane.Etcd = *etcd
		}
	}()

	go func() {
		defer wg.Done()
		if scheduler, err := c.GetSchedulerMetrics(timeRange); err == nil {
			controlPlane.Scheduler = *scheduler
		}
	}()

	go func() {
		defer wg.Done()
		if cm, err := c.GetControllerManagerMetrics(timeRange); err == nil {
			controlPlane.ControllerManager = *cm
		}
	}()

	wg.Wait()
	return controlPlane, nil
}

// GetAPIServerMetrics èŽ·å– API Server æŒ‡æ ‡ï¼ˆå®Œå–„ç‰ˆï¼‰
func (c *ClusterOperator) GetAPIServerMetrics(timeRange *types.TimeRange) (*types.APIServerMetrics, error) {
	apiServer := &types.APIServerMetrics{
		RequestsByVerb: []types.VerbMetrics{},
		RequestsByCode: types.StatusCodeDistribution{
			Status2xx: make(map[string]float64),
			Status3xx: make(map[string]float64),
			Status4xx: make(map[string]float64),
			Status5xx: make(map[string]float64),
		},
		RequestsByResource: []types.ResourceMetrics{},
		Trend:              []types.APIServerDataPoint{},
	}

	window := c.calculateRateWindow(timeRange)
	c.log.Infof("ðŸ“Š å¼€å§‹æŸ¥è¯¢ API Server æŒ‡æ ‡ (window: %s)", window)

	// ==================== åŸºç¡€æŒ‡æ ‡å’Œå»¶è¿Ÿ ====================
	tasks := []clusterQueryTask{
		{
			name:  "qps",
			query: fmt.Sprintf(`sum(rate(apiserver_request_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.RequestsPerSecond = results[0].Value
					c.log.Infof("âœ… API Server QPS: %.2f req/s", apiServer.RequestsPerSecond)
				}
				return nil
			},
		},
		{
			name:  "error_rate",
			query: fmt.Sprintf(`(sum(rate(apiserver_request_total{code=~"5.."}[%s])) or vector(0)) / (sum(rate(apiserver_request_total[%s])) or vector(1))`, window, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.ErrorRate = results[0].Value
					c.log.Infof("âœ… API Server Error Rate: %.2f%%", apiServer.ErrorRate*100)
				}
				return nil
			},
		},
		{
			name:  "p50",
			query: fmt.Sprintf(`histogram_quantile(0.50, sum(rate(apiserver_request_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.P50Latency = results[0].Value
					c.log.Infof("âœ… API Server P50 Latency: %.4f s", apiServer.P50Latency)
				}
				return nil
			},
		},
		{
			name:  "p95",
			query: fmt.Sprintf(`histogram_quantile(0.95, sum(rate(apiserver_request_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.P95Latency = results[0].Value
					c.log.Infof("âœ… API Server P95 Latency: %.4f s", apiServer.P95Latency)
				}
				return nil
			},
		},
		{
			name:  "p99",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.P99Latency = results[0].Value
					c.log.Infof("âœ… API Server P99 Latency: %.4f s", apiServer.P99Latency)
				}
				return nil
			},
		},

		// ==================== å¹¶å‘æŒ‡æ ‡ ====================
		{
			name:  "inflight_requests",
			query: `sum(apiserver_current_inflight_requests)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.CurrentInflightRequests = int64(results[0].Value)
					c.log.Infof("âœ… Current Inflight Requests: %d", apiServer.CurrentInflightRequests)
				}
				return nil
			},
		},
		{
			name:  "inflight_read",
			query: `sum(apiserver_current_inflight_requests{request_kind="readOnly"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.InflightReadRequests = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "inflight_mutating",
			query: `sum(apiserver_current_inflight_requests{request_kind="mutating"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.InflightMutatingRequests = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "longrunning",
			query: `sum(apiserver_longrunning_requests)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.LongRunningRequests = int64(results[0].Value)
					c.log.Infof("âœ… Long Running Requests: %d", apiServer.LongRunningRequests)
				}
				return nil
			},
		},
		{
			name:  "watch_count",
			query: `sum(apiserver_registered_watchers)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.WatchCount = int64(results[0].Value)
					c.log.Infof("âœ… Watch Count: %d", apiServer.WatchCount)
				}
				return nil
			},
		},

		// ==================== æ€§èƒ½æŒ‡æ ‡ ====================
		{
			name:  "request_dropped",
			query: fmt.Sprintf(`sum(increase(apiserver_dropped_requests_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.RequestDropped = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "request_timeout",
			query: fmt.Sprintf(`sum(increase(apiserver_request_timeout_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.RequestTimeout = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "response_size",
			query: fmt.Sprintf(`sum(rate(apiserver_response_sizes_sum[%s])) / sum(rate(apiserver_response_sizes_count[%s]))`, window, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.ResponseSizeBytes = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "webhook_duration",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(apiserver_admission_webhook_admission_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.WebhookDurationSeconds = results[0].Value
					c.log.Infof("âœ… Webhook Duration P99: %.4f s", apiServer.WebhookDurationSeconds)
				}
				return nil
			},
		},

		// ==================== è®¤è¯å’Œé‰´æƒ ====================
		{
			name:  "auth_attempts",
			query: fmt.Sprintf(`sum(rate(authentication_attempts[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.AuthenticationAttempts = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "auth_failures",
			query: fmt.Sprintf(`sum(rate(authentication_attempts{result="failure"}[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.AuthenticationFailures = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "authz_attempts",
			query: fmt.Sprintf(`sum(rate(apiserver_authorization_decisions_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.AuthorizationAttempts = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "authz_duration",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(apiserver_authorization_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.AuthorizationDuration = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "client_cert_expiration",
			query: `min(apiserver_client_certificate_expiration_seconds) / 86400`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					apiServer.ClientCertExpirationDays = results[0].Value
				}
				return nil
			},
		},

		// ==================== åˆ†ç±»ç»Ÿè®¡ ====================
		{
			name:  "verb",
			query: fmt.Sprintf(`sum(rate(apiserver_request_total[%s])) by (verb)`, window),
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if verb, ok := r.Metric["verb"]; ok {
						apiServer.RequestsByVerb = append(apiServer.RequestsByVerb, types.VerbMetrics{
							Verb:              verb,
							RequestsPerSecond: r.Value,
						})
					}
				}
				return nil
			},
		},
		{
			name:  "code",
			query: fmt.Sprintf(`sum(rate(apiserver_request_total[%s])) by (code)`, window),
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if code, ok := r.Metric["code"]; ok {
						c.addStatusCode(&apiServer.RequestsByCode, code, r.Value)
					}
				}
				return nil
			},
		},
		{
			name:  "resource",
			query: fmt.Sprintf(`topk(10, sum(rate(apiserver_request_total[%s])) by (resource))`, window),
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if resource, ok := r.Metric["resource"]; ok {
						apiServer.RequestsByResource = append(apiServer.RequestsByResource, types.ResourceMetrics{
							Resource:          resource,
							RequestsPerSecond: r.Value,
						})
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// ==================== è¶‹åŠ¿æ•°æ® ====================
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		c.log.Infof("ðŸ“Š æŸ¥è¯¢ API Server è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
			timeRange.Start.Format("2006-01-02 15:04:05"),
			timeRange.End.Format("2006-01-02 15:04:05"),
			step)

		var mu sync.Mutex
		rangeTasks := []clusterRangeTask{
			{
				name:  "qps_trend",
				query: fmt.Sprintf(`sum(rate(apiserver_request_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						apiServer.Trend = make([]types.APIServerDataPoint, len(results[0].Values))
						for i, v := range results[0].Values {
							apiServer.Trend[i] = types.APIServerDataPoint{
								Timestamp:         v.Timestamp,
								RequestsPerSecond: v.Value,
							}
						}
					}
					return nil
				},
			},
			{
				name:  "error_trend",
				query: fmt.Sprintf(`(sum(rate(apiserver_request_total{code=~"5.."}[%s])) or vector(0)) / (sum(rate(apiserver_request_total[%s])) or vector(1))`, window, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(apiServer.Trend) {
								apiServer.Trend[i].ErrorRate = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "p95_trend",
				query: fmt.Sprintf(`histogram_quantile(0.95, sum(rate(apiserver_request_duration_seconds_bucket[%s])) by (le))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(apiServer.Trend) {
								apiServer.Trend[i].P95Latency = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "inflight_trend",
				query: `sum(apiserver_current_inflight_requests)`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(apiServer.Trend) {
								apiServer.Trend[i].CurrentInflightRequests = int64(v.Value)
							}
						}
					}
					return nil
				},
			},
			{
				name:  "longrunning_trend",
				query: `sum(apiserver_longrunning_requests)`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(apiServer.Trend) {
								apiServer.Trend[i].LongRunningRequests = int64(v.Value)
							}
						}
					}
					return nil
				},
			},
		}

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
		c.log.Infof("ðŸ“ˆ API Server è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(apiServer.Trend))
	}

	return apiServer, nil
}

// ==================== Scheduler ä¼˜åŒ– ====================

// GetSchedulerMetrics èŽ·å– Scheduler æŒ‡æ ‡ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
// GetSchedulerMetrics èŽ·å– Scheduler æŒ‡æ ‡ï¼ˆå®Œå–„ç‰ˆï¼‰
func (c *ClusterOperator) GetSchedulerMetrics(timeRange *types.TimeRange) (*types.SchedulerMetrics, error) {
	scheduler := &types.SchedulerMetrics{
		FailureReasons: types.ScheduleFailureReasons{},
		PluginLatency:  types.SchedulerPluginLatency{},
		Trend:          []types.SchedulerDataPoint{},
	}
	window := c.calculateRateWindow(timeRange)

	c.log.Infof("ðŸ“Š å¼€å§‹æŸ¥è¯¢ Scheduler æŒ‡æ ‡ (window: %s)", window)

	// ==================== å°è¯•å¤šç§æŒ‡æ ‡ç‰ˆæœ¬ ====================
	// ä¸åŒ Kubernetes ç‰ˆæœ¬çš„æŒ‡æ ‡åç§°å¯èƒ½ä¸åŒ
	successRateQueries := []string{
		// Kubernetes 1.14+
		fmt.Sprintf(`sum(rate(scheduler_schedule_attempts_total{result="scheduled"}[%s])) / sum(rate(scheduler_schedule_attempts_total[%s]))`, window, window),
		// è€ç‰ˆæœ¬
		fmt.Sprintf(`sum(rate(scheduler_scheduling_algorithm_duration_seconds_count{result="scheduled"}[%s])) / sum(rate(scheduler_scheduling_algorithm_duration_seconds_count[%s]))`, window, window),
		// å¤‡ç”¨ï¼šæ ¹æ® Pod çŠ¶æ€æŽ¨ç®—
		`1 - (sum(kube_pod_status_phase{phase="Pending"}) / (sum(kube_pod_status_phase) or vector(1)))`,
	}

	latencyQueries := []string{
		fmt.Sprintf(`histogram_quantile(0.95, sum(rate(scheduler_scheduling_duration_seconds_bucket[%s])) by (le))`, window),
		fmt.Sprintf(`histogram_quantile(0.95, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket[%s])) by (le))`, window),
		fmt.Sprintf(`histogram_quantile(0.95, sum(rate(scheduler_binding_duration_seconds_bucket[%s])) by (le))`, window),
	}

	// ==================== åŸºç¡€æŒ‡æ ‡ ====================
	tasks := []clusterQueryTask{
		{
			name:  "schedule_attempts",
			query: fmt.Sprintf(`sum(rate(scheduler_schedule_attempts_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.ScheduleAttempts = results[0].Value
					c.log.Infof("âœ… Schedule Attempts: %.2f/s", scheduler.ScheduleAttempts)
				}
				return nil
			},
		},
		{
			name: "success_rate",
			query: func() string {
				for _, q := range successRateQueries {
					if result, err := c.query(q, nil); err == nil && len(result) > 0 {
						c.log.Infof("âœ… ä½¿ç”¨ Success Rate æŸ¥è¯¢: %s", q)
						return q
					}
				}
				return successRateQueries[0]
			}(),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.ScheduleSuccessRate = results[0].Value
					c.log.Infof("âœ… Schedule Success Rate: %.2f%%", scheduler.ScheduleSuccessRate*100)
				} else {
					scheduler.ScheduleSuccessRate = 1.0
					c.log.Errorf("âš ï¸  æ— æ³•èŽ·å– Success Rateï¼Œä½¿ç”¨é»˜è®¤å€¼ 100%%")
				}
				return nil
			},
		},
		{
			name:  "pending_pods",
			query: `sum(kube_pod_status_phase{phase="Pending"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PendingPods = int64(results[0].Value)
					c.log.Infof("âœ… Pending Pods: %d", scheduler.PendingPods)
				}
				return nil
			},
		},
		{
			name:  "unschedulable_pods",
			query: `sum(kube_pod_status_unschedulable)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.UnschedulablePods = int64(results[0].Value)
					c.log.Infof("âœ… Unschedulable Pods: %d", scheduler.UnschedulablePods)
				}
				return nil
			},
		},

		// ==================== å»¶è¿ŸæŒ‡æ ‡ ====================
		{
			name: "p50",
			query: func() string {
				for _, q := range latencyQueries {
					if result, err := c.query(q, nil); err == nil && len(result) > 0 {
						return strings.Replace(q, "0.95", "0.50", 1)
					}
				}
				return strings.Replace(latencyQueries[0], "0.95", "0.50", 1)
			}(),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.P50ScheduleLatency = results[0].Value
					c.log.Infof("âœ… Schedule P50 Latency: %.4f s", scheduler.P50ScheduleLatency)
				}
				return nil
			},
		},
		{
			name: "p95",
			query: func() string {
				for _, q := range latencyQueries {
					if result, err := c.query(q, nil); err == nil && len(result) > 0 {
						c.log.Infof("âœ… ä½¿ç”¨ Latency æŸ¥è¯¢: %s", q)
						return q
					}
				}
				return latencyQueries[0]
			}(),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.P95ScheduleLatency = results[0].Value
					c.log.Infof("âœ… Schedule P95 Latency: %.4f s", scheduler.P95ScheduleLatency)
				}
				return nil
			},
		},
		{
			name: "p99",
			query: func() string {
				for _, q := range latencyQueries {
					if result, err := c.query(q, nil); err == nil && len(result) > 0 {
						return strings.Replace(q, "0.95", "0.99", 1)
					}
				}
				return strings.Replace(latencyQueries[0], "0.95", "0.99", 1)
			}(),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.P99ScheduleLatency = results[0].Value
					c.log.Infof("âœ… Schedule P99 Latency: %.4f s", scheduler.P99ScheduleLatency)
				}
				return nil
			},
		},
		{
			name:  "binding_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.BindingLatency = results[0].Value
					c.log.Infof("âœ… Binding Latency P99: %.4f s", scheduler.BindingLatency)
				}
				return nil
			},
		},

		// ==================== è°ƒåº¦ç»“æžœ ====================
		{
			name:  "scheduled",
			query: fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="scheduled"}[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.ScheduledPods = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "failed",
			query: fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result!="scheduled"}[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.FailedScheduling = int64(results[0].Value)
					c.log.Infof("âœ… Failed Scheduling: %d", scheduler.FailedScheduling)
				}
				return nil
			},
		},
		{
			name:  "preemption_attempts",
			query: fmt.Sprintf(`sum(increase(scheduler_preemption_attempts_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PreemptionAttempts = int64(results[0].Value)
					c.log.Infof("âœ… Preemption Attempts: %d", scheduler.PreemptionAttempts)
				}
				return nil
			},
		},
		{
			name:  "preemption_victims",
			query: fmt.Sprintf(`sum(increase(scheduler_preemption_victims[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PreemptionVictims = int64(results[0].Value)
				}
				return nil
			},
		},

		// ==================== è°ƒåº¦é˜Ÿåˆ— ====================
		{
			name:  "queue_length",
			query: `sum(scheduler_pending_pods)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.SchedulingQueueLength = int64(results[0].Value)
					c.log.Infof("âœ… Scheduling Queue Length: %d", scheduler.SchedulingQueueLength)
				}
				return nil
			},
		},
		{
			name:  "active_queue",
			query: `sum(scheduler_queue_incoming_pods_total{event="Active"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.ActiveQueueLength = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "backoff_queue",
			query: `sum(scheduler_queue_incoming_pods_total{event="Backoff"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.BackoffQueueLength = int64(results[0].Value)
				}
				return nil
			},
		},

		// ==================== Framework æ’ä»¶å»¶è¿Ÿ ====================
		{
			name:  "filter_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(scheduler_framework_extension_point_duration_seconds_bucket{extension_point="Filter"}[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PluginLatency.FilterLatency = results[0].Value
					c.log.Infof("âœ… Filter Plugin Latency P99: %.4f s", scheduler.PluginLatency.FilterLatency)
				}
				return nil
			},
		},
		{
			name:  "score_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(scheduler_framework_extension_point_duration_seconds_bucket{extension_point="Score"}[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PluginLatency.ScoreLatency = results[0].Value
					c.log.Infof("âœ… Score Plugin Latency P99: %.4f s", scheduler.PluginLatency.ScoreLatency)
				}
				return nil
			},
		},
		{
			name:  "prebind_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(scheduler_framework_extension_point_duration_seconds_bucket{extension_point="PreBind"}[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PluginLatency.PreBindLatency = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "bind_latency_plugin",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(scheduler_framework_extension_point_duration_seconds_bucket{extension_point="Bind"}[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					scheduler.PluginLatency.BindLatency = results[0].Value
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// ==================== è°ƒåº¦å¤±è´¥åŽŸå› åˆ†ç±» ====================
	c.getScheduleFailureReasons(scheduler, window)

	// ==================== è¶‹åŠ¿æ•°æ® ====================
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		c.log.Infof("ðŸ“Š æŸ¥è¯¢ Scheduler è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
			timeRange.Start.Format("2006-01-02 15:04:05"),
			timeRange.End.Format("2006-01-02 15:04:05"),
			step)

		var mu sync.Mutex

		// é€‰æ‹©å¯ç”¨çš„æŸ¥è¯¢
		successQuery := successRateQueries[0]
		for _, q := range successRateQueries {
			if result, err := c.query(q, nil); err == nil && len(result) > 0 {
				successQuery = q
				break
			}
		}

		rangeTasks := []clusterRangeTask{
			{
				name:  "pending_pods_trend",
				query: `sum(kube_pod_status_phase{phase="Pending"})`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						scheduler.Trend = make([]types.SchedulerDataPoint, len(results[0].Values))
						for i, v := range results[0].Values {
							scheduler.Trend[i] = types.SchedulerDataPoint{
								Timestamp:   v.Timestamp,
								PendingPods: int64(v.Value),
							}
						}
					}
					return nil
				},
			},
			{
				name:  "success_rate_trend",
				query: successQuery,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(scheduler.Trend) {
								scheduler.Trend[i].ScheduleSuccessRate = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name: "p95_latency_trend",
				query: func() string {
					for _, q := range latencyQueries {
						if result, err := c.query(q, nil); err == nil && len(result) > 0 {
							return q
						}
					}
					return latencyQueries[0]
				}(),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(scheduler.Trend) {
								scheduler.Trend[i].P95ScheduleLatency = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "attempts_trend",
				query: fmt.Sprintf(`sum(rate(scheduler_schedule_attempts_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(scheduler.Trend) {
								scheduler.Trend[i].ScheduleAttempts = v.Value
							}
						}
					}
					return nil
				},
			},
		}

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
		c.log.Infof("ðŸ“ˆ Scheduler è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(scheduler.Trend))
	}

	return scheduler, nil
}

// getScheduleFailureReasons èŽ·å–è°ƒåº¦å¤±è´¥åŽŸå› åˆ†ç±»
func (c *ClusterOperator) getScheduleFailureReasons(scheduler *types.SchedulerMetrics, window string) {
	// å°è¯•å¤šç§æŒ‡æ ‡æ ¼å¼
	reasonQueries := map[string]string{
		"InsufficientCPU":    fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason="Insufficient cpu"}[%s]))`, window),
		"InsufficientMemory": fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason="Insufficient memory"}[%s]))`, window),
		"NodeAffinity":       fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason=~".*node.*affinity.*"}[%s]))`, window),
		"PodAffinity":        fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason=~".*pod.*affinity.*"}[%s]))`, window),
		"Taint":              fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason=~".*taint.*"}[%s]))`, window),
		"VolumeBinding":      fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason=~".*volume.*"}[%s]))`, window),
		"NoNodesAvailable":   fmt.Sprintf(`sum(increase(scheduler_schedule_attempts_total{result="error",profile="default",reason="No nodes available"}[%s]))`, window),
	}

	// å¤‡ç”¨æŸ¥è¯¢ï¼ˆåŸºäºŽ Pod æ¡ä»¶ï¼‰
	backupQueries := map[string]string{
		"InsufficientCPU":  `sum(kube_pod_status_scheduled_time == 0) and on(pod) kube_pod_condition{condition="PodScheduled",status="false",reason="Unschedulable"} * on(pod) kube_pod_info{created_by_kind="ReplicaSet"}`,
		"NoNodesAvailable": `sum(kube_pod_status_unschedulable)`,
	}

	c.log.Infof("ðŸ“Š æŸ¥è¯¢è°ƒåº¦å¤±è´¥åŽŸå› åˆ†ç±»")

	tasks := []clusterQueryTask{}
	for name, query := range reasonQueries {
		reasonName := name
		reasonQuery := query
		tasks = append(tasks, clusterQueryTask{
			name:  "reason_" + name,
			query: reasonQuery,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					count := int64(results[0].Value)
					switch reasonName {
					case "InsufficientCPU":
						scheduler.FailureReasons.InsufficientCPU = count
					case "InsufficientMemory":
						scheduler.FailureReasons.InsufficientMemory = count
					case "NodeAffinity":
						scheduler.FailureReasons.NodeAffinity = count
					case "PodAffinity":
						scheduler.FailureReasons.PodAffinity = count
					case "Taint":
						scheduler.FailureReasons.Taint = count
					case "VolumeBinding":
						scheduler.FailureReasons.VolumeBinding = count
					case "NoNodesAvailable":
						scheduler.FailureReasons.NoNodesAvailable = count
					}
					if count > 0 {
						c.log.Infof("âœ… Failure Reason [%s]: %d", reasonName, count)
					}
				}
				return nil
			},
		})
	}

	c.executeParallelQueries(tasks)

	// å¦‚æžœä¸»è¦æŸ¥è¯¢éƒ½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æŸ¥è¯¢
	if scheduler.FailureReasons.InsufficientCPU == 0 && scheduler.FailureReasons.NoNodesAvailable == 0 {
		c.log.Errorf("âš ï¸  è°ƒåº¦å¤±è´¥åŽŸå› æŒ‡æ ‡ä¸å¯ç”¨ï¼Œå°è¯•å¤‡ç”¨æŸ¥è¯¢")
		for name, query := range backupQueries {
			if results, err := c.query(query, nil); err == nil && len(results) > 0 {
				count := int64(results[0].Value)
				if name == "NoNodesAvailable" {
					scheduler.FailureReasons.NoNodesAvailable = count
				}
			}
		}
	}
}

// ==================== Controller Manager ä¼˜åŒ– ====================

// GetControllerManagerMetrics èŽ·å– Controller Manager æŒ‡æ ‡ï¼ˆä¿®å¤ç‰ˆï¼‰
func (c *ClusterOperator) GetControllerManagerMetrics(timeRange *types.TimeRange) (*types.ControllerManagerMetrics, error) {
	cm := &types.ControllerManagerMetrics{
		QueueLatency:     types.QueueLatencyMetrics{},
		WorkQueueMetrics: types.ControllerWorkQueueMetrics{},
		ReconcileLatency: types.ControllerReconcileLatency{},
		Trend:            []types.ControllerManagerDataPoint{},
	}
	window := c.calculateRateWindow(timeRange)

	c.log.Infof("ðŸ“Š å¼€å§‹æŸ¥è¯¢ Controller Manager æŒ‡æ ‡ (window: %s)", window)

	// å…ˆæŸ¥è¯¢æ‰€æœ‰å¯ç”¨çš„workqueueåç§°
	allWorkqueueQuery := `workqueue_depth`
	allWorkqueues, err := c.query(allWorkqueueQuery, nil)
	if err != nil {
		c.log.Errorf("âš ï¸  æ— æ³•æŸ¥è¯¢ workqueue_depth: %v", err)
	}

	// æå–æ‰€æœ‰å¯ç”¨çš„ name æ ‡ç­¾
	availableNames := make(map[string]float64)
	if len(allWorkqueues) > 0 {
		for _, result := range allWorkqueues {
			if name, ok := result.Metric["name"]; ok {
				availableNames[name] += result.Value
			}
		}
		c.log.Infof("âœ… å‘çŽ°çš„ workqueue åç§°:")
		for name, depth := range availableNames {
			c.log.Infof("  - %s: %.0f", name, depth)
		}
	}

	// ==================== Leader é€‰ä¸¾ ====================
	tasks := []clusterQueryTask{
		{
			name:  "is_leader",
			query: `max(leader_election_master_status{name="kube-controller-manager"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.IsLeader = results[0].Value > 0
					c.log.Infof("âœ… Controller Manager Is Leader: %v", cm.IsLeader)
				}
				return nil
			},
		},
		{
			name:  "leader_changes",
			query: `sum(increase(leader_election_master_status{name="kube-controller-manager"}[1h]))`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.LeaderChanges = int64(results[0].Value)
					if cm.LeaderChanges > 0 {
						c.log.Infof("âš ï¸  Leader Changes (1h): %d", cm.LeaderChanges)
					}
				}
				return nil
			},
		},

		// ==================== å·¥ä½œé˜Ÿåˆ—æ·±åº¦ï¼ˆç›´æŽ¥ä½¿ç”¨å®žé™…åç§°ï¼‰====================
		{
			name:  "deployment_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"deployment"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.DeploymentQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… Deployment Queue Depth: %d", cm.DeploymentQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "replicaset_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"replicaset"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReplicaSetQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… ReplicaSet Queue Depth: %d", cm.ReplicaSetQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "statefulset_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"statefulset"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.StatefulSetQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… StatefulSet Queue Depth: %d", cm.StatefulSetQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "daemonset_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"daemonset"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.DaemonSetQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… DaemonSet Queue Depth: %d", cm.DaemonSetQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "job_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"job"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.JobQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… Job Queue Depth: %d", cm.JobQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "node_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"node", "nodelifecycle"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.NodeQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… Node Queue Depth: %d", cm.NodeQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "service_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"service"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ServiceQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… Service Queue Depth: %d", cm.ServiceQueueDepth)
				}
				return nil
			},
		},
		{
			name:  "endpoint_queue",
			query: c.findWorkqueueQuery(availableNames, []string{"endpoint", "endpointslice"}),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.EndpointQueueDepth = int64(results[0].Value)
					c.log.Infof("âœ… Endpoint Queue Depth: %d", cm.EndpointQueueDepth)
				}
				return nil
			},
		},

		// ==================== é˜Ÿåˆ—å»¶è¿Ÿ ====================
		{
			name:  "queue_duration",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(workqueue_queue_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.QueueLatency.QueueDuration = results[0].Value
					c.log.Infof("âœ… Queue Duration P99: %.4f s", cm.QueueLatency.QueueDuration)
				}
				return nil
			},
		},
		{
			name:  "work_duration",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(workqueue_work_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.QueueLatency.WorkDuration = results[0].Value
					c.log.Infof("âœ… Work Duration P99: %.4f s", cm.QueueLatency.WorkDuration)
				}
				return nil
			},
		},

		// ==================== å·¥ä½œé˜Ÿåˆ—ç»Ÿè®¡ ====================
		{
			name:  "adds_rate",
			query: fmt.Sprintf(`sum(rate(workqueue_adds_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.WorkQueueMetrics.AddsRate = results[0].Value
					c.log.Infof("âœ… Workqueue Adds Rate: %.2f items/s", cm.WorkQueueMetrics.AddsRate)
				}
				return nil
			},
		},
		{
			name:  "depth_total",
			query: `sum(workqueue_depth)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.WorkQueueMetrics.DepthTotal = int64(results[0].Value)
					c.log.Infof("âœ… Total Queue Depth: %d", cm.WorkQueueMetrics.DepthTotal)
				}
				return nil
			},
		},
		{
			name:  "unfinished_work",
			query: `sum(workqueue_unfinished_work_seconds)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.WorkQueueMetrics.UnfinishedWork = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "longest_running",
			query: `max(workqueue_longest_running_processor_seconds)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.WorkQueueMetrics.LongestRunning = results[0].Value
					if cm.WorkQueueMetrics.LongestRunning > 60 {
						c.log.Errorf("âš ï¸  Longest Running Work: %.2f s", cm.WorkQueueMetrics.LongestRunning)
					}
				}
				return nil
			},
		},
		{
			name:  "retries_rate",
			query: fmt.Sprintf(`sum(rate(workqueue_retries_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.WorkQueueMetrics.RetriesRate = results[0].Value
					cm.RetryRate = results[0].Value
					c.log.Infof("âœ… Retry Rate: %.2f items/s", cm.RetryRate)
				}
				return nil
			},
		},

		// ==================== é”™è¯¯å’Œé‡è¯• ====================
		{
			name:  "sync_errors",
			query: fmt.Sprintf(`sum(increase(controller_runtime_reconcile_errors_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.TotalSyncErrors = int64(results[0].Value)
					if cm.TotalSyncErrors > 0 {
						c.log.Infof("âš ï¸  Total Sync Errors: %d", cm.TotalSyncErrors)
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// ==================== æŽ§åˆ¶å™¨åè°ƒå»¶è¿Ÿ ====================
	c.getReconcileLatency(cm, window, availableNames)

	// ==================== è¶‹åŠ¿æ•°æ® ====================
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		c.log.Infof("ðŸ“Š æŸ¥è¯¢ Controller Manager è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
			timeRange.Start.Format("2006-01-02 15:04:05"),
			timeRange.End.Format("2006-01-02 15:04:05"),
			step)

		var mu sync.Mutex
		rangeTasks := []clusterRangeTask{}

		// æ€»é˜Ÿåˆ—æ·±åº¦è¶‹åŠ¿ï¼ˆä¿åº•ç­–ç•¥ï¼‰
		rangeTasks = append(rangeTasks, clusterRangeTask{
			name:  "total_queue_trend",
			query: `sum(workqueue_depth)`,
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					c.log.Infof("âœ… æ€»é˜Ÿåˆ—è¶‹åŠ¿: %d ä¸ªæ•°æ®ç‚¹", len(results[0].Values))
					cm.Trend = make([]types.ControllerManagerDataPoint, len(results[0].Values))
					for i, v := range results[0].Values {
						cm.Trend[i] = types.ControllerManagerDataPoint{
							Timestamp:       v.Timestamp,
							TotalQueueDepth: int64(v.Value),
						}
					}
				}
				return nil
			},
		})

		// Deploymenté˜Ÿåˆ—è¶‹åŠ¿
		deploymentQuery := c.findWorkqueueQuery(availableNames, []string{"deployment"})
		if deploymentQuery != "" {
			rangeTasks = append(rangeTasks, clusterRangeTask{
				name:  "deployment_queue_trend",
				query: deploymentQuery,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						c.log.Infof("âœ… Deployment è¶‹åŠ¿: %d ä¸ªæ•°æ®ç‚¹", len(results[0].Values))
						for i, v := range results[0].Values {
							if i < len(cm.Trend) {
								cm.Trend[i].DeploymentQueueDepth = int64(v.Value)
							}
						}
					}
					return nil
				},
			})
		}

		// ReplicaSeté˜Ÿåˆ—è¶‹åŠ¿
		replicasetQuery := c.findWorkqueueQuery(availableNames, []string{"replicaset"})
		if replicasetQuery != "" {
			rangeTasks = append(rangeTasks, clusterRangeTask{
				name:  "replicaset_queue_trend",
				query: replicasetQuery,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						c.log.Infof("âœ… ReplicaSet è¶‹åŠ¿: %d ä¸ªæ•°æ®ç‚¹", len(results[0].Values))
						for i, v := range results[0].Values {
							if i < len(cm.Trend) {
								cm.Trend[i].ReplicaSetQueueDepth = int64(v.Value)
							}
						}
					}
					return nil
				},
			})
		}

		// StatefulSeté˜Ÿåˆ—è¶‹åŠ¿
		statefulsetQuery := c.findWorkqueueQuery(availableNames, []string{"statefulset"})
		if statefulsetQuery != "" {
			rangeTasks = append(rangeTasks, clusterRangeTask{
				name:  "statefulset_queue_trend",
				query: statefulsetQuery,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						c.log.Infof("âœ… StatefulSet è¶‹åŠ¿: %d ä¸ªæ•°æ®ç‚¹", len(results[0].Values))
						for i, v := range results[0].Values {
							if i < len(cm.Trend) {
								cm.Trend[i].StatefulSetQueueDepth = int64(v.Value)
							}
						}
					}
					return nil
				},
			})
		}

		// é‡è¯•çŽ‡è¶‹åŠ¿
		rangeTasks = append(rangeTasks, clusterRangeTask{
			name:  "retry_rate_trend",
			query: fmt.Sprintf(`sum(rate(workqueue_retries_total[%s]))`, window),
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					c.log.Infof("âœ… é‡è¯•çŽ‡è¶‹åŠ¿: %d ä¸ªæ•°æ®ç‚¹", len(results[0].Values))
					for i, v := range results[0].Values {
						if i < len(cm.Trend) {
							cm.Trend[i].RetryRate = v.Value
						}
					}
				}
				return nil
			},
		})

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
		c.log.Infof("ðŸ“ˆ Controller Manager è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(cm.Trend))
	}

	return cm, nil
}

// findWorkqueueQuery æ™ºèƒ½æŸ¥æ‰¾workqueueæŸ¥è¯¢
func (c *ClusterOperator) findWorkqueueQuery(availableNames map[string]float64, keywords []string) string {
	if len(availableNames) == 0 {
		c.log.Errorf("âš ï¸  æ²¡æœ‰å¯ç”¨çš„ workqueue")
		return ""
	}

	// ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…
	for _, keyword := range keywords {
		for name := range availableNames {
			if strings.EqualFold(name, keyword) {
				query := fmt.Sprintf(`sum(workqueue_depth{name="%s"})`, name)
				c.log.Infof("âœ… ç²¾ç¡®åŒ¹é… [%s] -> %s", keyword, name)
				return query
			}
		}
	}

	// ç­–ç•¥2ï¼šåŒ…å«åŒ¹é…ï¼ˆä¼˜å…ˆæœ€çŸ­ï¼‰
	var bestMatch string
	minLen := 999
	for name := range availableNames {
		nameLower := strings.ToLower(name)
		for _, keyword := range keywords {
			if strings.Contains(nameLower, strings.ToLower(keyword)) {
				if len(name) < minLen {
					bestMatch = name
					minLen = len(name)
				}
			}
		}
	}
	if bestMatch != "" {
		query := fmt.Sprintf(`sum(workqueue_depth{name="%s"})`, bestMatch)
		c.log.Infof("âœ… åŒ…å«åŒ¹é… %v -> %s", keywords, bestMatch)
		return query
	}

	// ç­–ç•¥3ï¼šæ­£åˆ™åŒ¹é…ï¼ˆæœ€åŽæ‰‹æ®µï¼‰
	pattern := strings.Join(keywords, "|")
	query := fmt.Sprintf(`sum(workqueue_depth{name=~"(?i).*(%s).*"})`, pattern)
	c.log.Errorf("âš ï¸  ä½¿ç”¨æ­£åˆ™åŒ¹é… %v: %s", keywords, query)
	return query
}

// getReconcileLatency èŽ·å–æŽ§åˆ¶å™¨åè°ƒå»¶è¿Ÿ
func (c *ClusterOperator) getReconcileLatency(cm *types.ControllerManagerMetrics, window string, availableNames map[string]float64) {
	c.log.Infof("ðŸ“Š æŸ¥è¯¢æŽ§åˆ¶å™¨åè°ƒå»¶è¿Ÿ")

	tasks := []clusterQueryTask{
		{
			name:  "deployment_reconcile",
			query: c.buildReconcileQuery(availableNames, []string{"deployment"}, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReconcileLatency.DeploymentP99 = results[0].Value
					c.log.Infof("âœ… Deployment Reconcile P99: %.4f s", cm.ReconcileLatency.DeploymentP99)
				}
				return nil
			},
		},
		{
			name:  "replicaset_reconcile",
			query: c.buildReconcileQuery(availableNames, []string{"replicaset"}, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReconcileLatency.ReplicaSetP99 = results[0].Value
					c.log.Infof("âœ… ReplicaSet Reconcile P99: %.4f s", cm.ReconcileLatency.ReplicaSetP99)
				}
				return nil
			},
		},
		{
			name:  "statefulset_reconcile",
			query: c.buildReconcileQuery(availableNames, []string{"statefulset"}, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReconcileLatency.StatefulSetP99 = results[0].Value
					c.log.Infof("âœ… StatefulSet Reconcile P99: %.4f s", cm.ReconcileLatency.StatefulSetP99)
				}
				return nil
			},
		},
		{
			name:  "daemonset_reconcile",
			query: c.buildReconcileQuery(availableNames, []string{"daemonset"}, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReconcileLatency.DaemonSetP99 = results[0].Value
					c.log.Infof("âœ… DaemonSet Reconcile P99: %.4f s", cm.ReconcileLatency.DaemonSetP99)
				}
				return nil
			},
		},
		{
			name:  "job_reconcile",
			query: c.buildReconcileQuery(availableNames, []string{"job"}, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					cm.ReconcileLatency.JobP99 = results[0].Value
					c.log.Infof("âœ… Job Reconcile P99: %.4f s", cm.ReconcileLatency.JobP99)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
}

// buildReconcileQuery æž„å»ºåè°ƒå»¶è¿ŸæŸ¥è¯¢
func (c *ClusterOperator) buildReconcileQuery(availableNames map[string]float64, keywords []string, window string) string {
	// å…ˆå°è¯• controller_runtime_reconcile_time_seconds
	for _, keyword := range keywords {
		query := fmt.Sprintf(`histogram_quantile(0.99, sum(rate(controller_runtime_reconcile_time_seconds_bucket{controller=~"(?i).*%s.*"}[%s])) by (le))`, keyword, window)
		results, err := c.query(query, nil)
		if err == nil && len(results) > 0 {
			c.log.Infof("âœ… ä½¿ç”¨ controller_runtime æŒ‡æ ‡: %s", keyword)
			return query
		}
	}

	// é™çº§ï¼šä½¿ç”¨workqueue_work_duration
	matchedName := ""
	for _, keyword := range keywords {
		for name := range availableNames {
			if strings.Contains(strings.ToLower(name), strings.ToLower(keyword)) {
				matchedName = name
				break
			}
		}
		if matchedName != "" {
			break
		}
	}

	if matchedName != "" {
		query := fmt.Sprintf(`histogram_quantile(0.99, sum(rate(workqueue_work_duration_seconds_bucket{name="%s"}[%s])) by (le))`, matchedName, window)
		c.log.Infof("âœ… ä½¿ç”¨ workqueue_work_duration: %s", matchedName)
		return query
	}

	// æœ€åŽé™çº§
	c.log.Errorf("âš ï¸  æ— æ³•æž„å»º reconcile æŸ¥è¯¢: %v", keywords)
	return fmt.Sprintf(`histogram_quantile(0.99, sum(rate(workqueue_work_duration_seconds_bucket[%s])) by (le))`, window)
}

// ==================== è¾…åŠ©ç»“æž„ä½“å’Œå‡½æ•° ====================

// WorkqueueInfo å­˜å‚¨workqueueè¯Šæ–­ä¿¡æ¯
type WorkqueueInfo struct {
	available      bool                       // workqueue_depth æŒ‡æ ‡æ˜¯å¦å¯ç”¨
	metricName     string                     // å®žé™…çš„æŒ‡æ ‡åç§°
	nameMap        map[string]float64         // name -> current depth
	allMetrics     []types.InstantQueryResult // æ‰€æœ‰åŽŸå§‹æŒ‡æ ‡
	matchedQueries map[string]string          // keyword -> matched queue name
}

// diagnoseWorkqueueMetrics è¯Šæ–­workqueueæŒ‡æ ‡å¯ç”¨æ€§
func (c *ClusterOperator) diagnoseWorkqueueMetrics() *WorkqueueInfo {
	info := &WorkqueueInfo{
		nameMap:        make(map[string]float64),
		matchedQueries: make(map[string]string),
	}

	c.log.Infof("ðŸ” å¼€å§‹è¯Šæ–­ workqueue æŒ‡æ ‡...")

	// 1. å°è¯•æŸ¥è¯¢ä¸åŒçš„æŒ‡æ ‡åç§°
	possibleMetrics := []string{
		"workqueue_depth",
		"workqueue_queue_length",
		"workqueue_current_depth",
	}

	var results []types.InstantQueryResult
	var err error

	for _, metric := range possibleMetrics {
		results, err = c.query(metric, nil)
		if err == nil && len(results) > 0 {
			info.available = true
			info.metricName = metric
			info.allMetrics = results
			c.log.Infof("âœ… æ‰¾åˆ°å¯ç”¨æŒ‡æ ‡: %s (å…± %d ä¸ªæ—¶åº)", metric, len(results))
			break
		}
	}

	if !info.available {
		c.log.Errorf("âŒ æœªæ‰¾åˆ°ä»»ä½• workqueue depth æŒ‡æ ‡")
		return info
	}

	// 2. è§£æžæ‰€æœ‰nameæ ‡ç­¾
	for _, result := range results {
		name, ok := result.Metric["name"]
		if !ok {
			c.log.Errorf("âš ï¸  å‘çŽ°æ²¡æœ‰nameæ ‡ç­¾çš„æŒ‡æ ‡: %+v", result.Metric)
			continue
		}
		info.nameMap[name] += result.Value
	}

	// 3. å°è¯•åŒ¹é…å¸¸è§controller
	keywords := []string{
		"deployment",
		"replicaset",
		"statefulset",
		"daemonset",
		"job",
		"node",
		"service",
		"endpoint",
	}

	c.log.Infof("ðŸ”Ž å¼€å§‹åŒ¹é…åˆ†æž:")
	for _, keyword := range keywords {
		if matched := c.findBestMatch(info.nameMap, keyword); matched != "" {
			info.matchedQueries[keyword] = matched
			c.log.Infof("  âœ… %s -> %s", keyword, matched)
		} else {
			c.log.Errorf("  âŒ %s æœªåŒ¹é…åˆ°ä»»ä½•é˜Ÿåˆ—", keyword)
		}
	}

	return info
}

// findBestMatch æŸ¥æ‰¾æœ€ä½³åŒ¹é…çš„workqueueåç§°
func (c *ClusterOperator) findBestMatch(nameMap map[string]float64, keyword string) string {
	keywordLower := strings.ToLower(keyword)

	// ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
	for name := range nameMap {
		if strings.EqualFold(name, keyword) {
			return name
		}
	}

	// ç­–ç•¥2ï¼šå®Œå…¨åŒ…å«åŒ¹é…ï¼ˆä¼˜å…ˆæœ€çŸ­ï¼‰
	var candidates []string
	for name := range nameMap {
		nameLower := strings.ToLower(name)
		if nameLower == keywordLower {
			return name
		}
		if strings.Contains(nameLower, keywordLower) {
			candidates = append(candidates, name)
		}
	}

	if len(candidates) > 0 {
		// è¿”å›žæœ€çŸ­çš„åŒ¹é…ï¼ˆæœ€ç²¾ç¡®ï¼‰
		shortest := candidates[0]
		for _, c := range candidates[1:] {
			if len(c) < len(shortest) {
				shortest = c
			}
		}
		return shortest
	}

	// ç­–ç•¥3ï¼šåŽç¼€åŒ¹é… (xxx_controller)
	suffix := keyword + "_controller"
	for name := range nameMap {
		if strings.HasSuffix(strings.ToLower(name), suffix) {
			return name
		}
	}

	// ç­–ç•¥4ï¼šå‰ç¼€åŒ¹é… (controller-xxx)
	prefix := "controller-" + keyword
	for name := range nameMap {
		if strings.HasPrefix(strings.ToLower(name), prefix) {
			return name
		}
	}

	// ç­–ç•¥5ï¼šåŒ…å«åŒ¹é…ï¼ˆå…è®¸ä¸‹åˆ’çº¿ã€æ¨ªçº¿åˆ†éš”ï¼‰
	patterns := []string{
		keyword + "_",
		keyword + "-",
		"_" + keyword,
		"-" + keyword,
	}
	for name := range nameMap {
		nameLower := strings.ToLower(name)
		for _, pattern := range patterns {
			if strings.Contains(nameLower, pattern) {
				return name
			}
		}
	}

	return ""
}

// buildWorkqueueQueryV2 æž„å»ºworkqueueæŸ¥è¯¢ï¼ˆæ”¹è¿›ç‰ˆï¼‰
func (c *ClusterOperator) buildWorkqueueQueryV2(info *WorkqueueInfo, keywords []string) (string, bool) {
	if !info.available {
		c.log.Errorf("âš ï¸  workqueue æŒ‡æ ‡ä¸å¯ç”¨")
		return "", false
	}

	// å°è¯•ä»Žå·²åŒ¹é…çš„æŸ¥è¯¢ä¸­æŸ¥æ‰¾
	for _, keyword := range keywords {
		if matched, ok := info.matchedQueries[keyword]; ok {
			query := fmt.Sprintf(`sum(%s{name="%s"})`, info.metricName, matched)
			return query, true
		}
	}

	// é™çº§ï¼šå°è¯•åŠ¨æ€åŒ¹é…
	for _, keyword := range keywords {
		if matched := c.findBestMatch(info.nameMap, keyword); matched != "" {
			query := fmt.Sprintf(`sum(%s{name="%s"})`, info.metricName, matched)
			c.log.Infof("âœ… åŠ¨æ€åŒ¹é… %s -> %s", keyword, matched)
			return query, true
		}
	}

	// æœ€åŽé™çº§ï¼šæ­£åˆ™åŒ¹é…
	pattern := strings.Join(keywords, "|")
	query := fmt.Sprintf(`sum(%s{name=~"(?i).*(%s).*"})`, info.metricName, pattern)
	c.log.Errorf("âš ï¸  ä½¿ç”¨æ­£åˆ™åŒ¹é…: %s", query)

	// éªŒè¯æ­£åˆ™æŸ¥è¯¢æ˜¯å¦æœ‰ç»“æžœ
	if results, err := c.query(query, nil); err == nil && len(results) > 0 {
		return query, true
	}

	return query, false
}

// selectAvailableQuery é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æŸ¥è¯¢
func (c *ClusterOperator) selectAvailableQuery(queries []string) string {
	for i, query := range queries {
		results, err := c.query(query, nil)
		if err == nil && len(results) > 0 {
			if i > 0 {
				c.log.Infof("âœ… ä½¿ç”¨å¤‡é€‰æŸ¥è¯¢[%d]: %s", i, query)
			}
			return query
		}
	}
	c.log.Errorf("âš ï¸  æ‰€æœ‰æŸ¥è¯¢éƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª: %s", queries[0])
	return queries[0]
}

// getReconcileLatencyV2 èŽ·å–æŽ§åˆ¶å™¨åè°ƒå»¶è¿Ÿï¼ˆæ”¹è¿›ç‰ˆï¼‰
func (c *ClusterOperator) getReconcileLatencyV2(cm *types.ControllerManagerMetrics, window string, info *WorkqueueInfo) {
	c.log.Infof("ðŸ“Š æŸ¥è¯¢æŽ§åˆ¶å™¨åè°ƒå»¶è¿Ÿ")

	// æŽ§åˆ¶å™¨é…ç½®
	configs := []struct {
		keywords []string
		target   *float64
		name     string
	}{
		{[]string{"deployment"}, &cm.ReconcileLatency.DeploymentP99, "Deployment"},
		{[]string{"replicaset"}, &cm.ReconcileLatency.ReplicaSetP99, "ReplicaSet"},
		{[]string{"statefulset"}, &cm.ReconcileLatency.StatefulSetP99, "StatefulSet"},
		{[]string{"daemonset"}, &cm.ReconcileLatency.DaemonSetP99, "DaemonSet"},
		{[]string{"job"}, &cm.ReconcileLatency.JobP99, "Job"},
	}

	tasks := []clusterQueryTask{}

	for _, config := range configs {
		query := c.buildReconcileLatencyQuery(config.keywords, window, info)
		target := config.target
		name := config.name

		tasks = append(tasks, clusterQueryTask{
			name:  strings.ToLower(name) + "_reconcile",
			query: query,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					*target = results[0].Value
					c.log.Infof("âœ… %s Reconcile P99: %.4f s", name, *target)
				}
				return nil
			},
		})
	}

	c.executeParallelQueries(tasks)
}

// buildReconcileLatencyQuery æž„å»ºåè°ƒå»¶è¿ŸæŸ¥è¯¢
func (c *ClusterOperator) buildReconcileLatencyQuery(keywords []string, window string, info *WorkqueueInfo) string {
	// å°è¯•å¤šç§æŒ‡æ ‡æ ¼å¼
	queries := []string{}

	// æ ¼å¼1ï¼šcontroller_runtime_reconcile_time_seconds
	for _, keyword := range keywords {
		queries = append(queries,
			fmt.Sprintf(`histogram_quantile(0.99, sum(rate(controller_runtime_reconcile_time_seconds_bucket{controller=~"(?i).*%s.*"}[%s])) by (le))`, keyword, window),
		)
	}

	// æ ¼å¼2ï¼šä½¿ç”¨workqueue_work_durationä½œä¸ºè¿‘ä¼¼å€¼
	if info.available {
		for _, keyword := range keywords {
			if matched := c.findBestMatch(info.nameMap, keyword); matched != "" {
				queries = append(queries,
					fmt.Sprintf(`histogram_quantile(0.99, sum(rate(workqueue_work_duration_seconds_bucket{name="%s"}[%s])) by (le))`, matched, window),
				)
			}
		}
	}

	return c.selectAvailableQuery(queries)
}

// queryControllerManagerTrendV2 æŸ¥è¯¢è¶‹åŠ¿æ•°æ®ï¼ˆæ”¹è¿›ç‰ˆï¼‰
func (c *ClusterOperator) queryControllerManagerTrendV2(cm *types.ControllerManagerMetrics, timeRange *types.TimeRange, window string, info *WorkqueueInfo) {
	step := timeRange.Step
	if step == "" {
		step = c.calculateStep(timeRange.Start, timeRange.End)
	}

	c.log.Infof("ðŸ“Š æŸ¥è¯¢ Controller Manager è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
		timeRange.Start.Format("2006-01-02 15:04:05"),
		timeRange.End.Format("2006-01-02 15:04:05"),
		step)

	var mu sync.Mutex
	rangeTasks := []clusterRangeTask{}

	// å¦‚æžœæœ‰deploymenté˜Ÿåˆ—ï¼ŒæŸ¥è¯¢å…¶è¶‹åŠ¿
	if query, found := c.buildWorkqueueQueryV2(info, []string{"deployment"}); found {
		rangeTasks = append(rangeTasks, clusterRangeTask{
			name:  "deployment_queue_trend",
			query: query,
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					cm.Trend = make([]types.ControllerManagerDataPoint, len(results[0].Values))
					for i, v := range results[0].Values {
						cm.Trend[i] = types.ControllerManagerDataPoint{
							Timestamp:            v.Timestamp,
							DeploymentQueueDepth: int64(v.Value),
						}
					}
				}
				return nil
			},
		})

		// ReplicaSetè¶‹åŠ¿
		if query, found := c.buildWorkqueueQueryV2(info, []string{"replicaset"}); found {
			rangeTasks = append(rangeTasks, clusterRangeTask{
				name:  "replicaset_queue_trend",
				query: query,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(cm.Trend) {
								cm.Trend[i].ReplicaSetQueueDepth = int64(v.Value)
							}
						}
					}
					return nil
				},
			})
		}

		// StatefulSetè¶‹åŠ¿
		if query, found := c.buildWorkqueueQueryV2(info, []string{"statefulset"}); found {
			rangeTasks = append(rangeTasks, clusterRangeTask{
				name:  "statefulset_queue_trend",
				query: query,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(cm.Trend) {
								cm.Trend[i].StatefulSetQueueDepth = int64(v.Value)
							}
						}
					}
					return nil
				},
			})
		}
	}

	// æ€»é˜Ÿåˆ—æ·±åº¦è¶‹åŠ¿
	if info.available {
		rangeTasks = append(rangeTasks, clusterRangeTask{
			name:  "total_queue_trend",
			query: fmt.Sprintf(`sum(%s)`, info.metricName),
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					for i, v := range results[0].Values {
						if i < len(cm.Trend) {
							cm.Trend[i].TotalQueueDepth = int64(v.Value)
						}
					}
				}
				return nil
			},
		})
	}

	// é‡è¯•çŽ‡è¶‹åŠ¿
	rangeTasks = append(rangeTasks, clusterRangeTask{
		name:  "retry_rate_trend",
		query: fmt.Sprintf(`sum(rate(workqueue_retries_total[%s]))`, window),
		f: func(results []types.RangeQueryResult) error {
			if len(results) > 0 && len(results[0].Values) > 0 {
				mu.Lock()
				defer mu.Unlock()
				for i, v := range results[0].Values {
					if i < len(cm.Trend) {
						cm.Trend[i].RetryRate = v.Value
					}
				}
			}
			return nil
		},
	})

	if len(rangeTasks) > 0 {
		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
		c.log.Infof("ðŸ“ˆ Controller Manager è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(cm.Trend))
	} else {
		c.log.Errorf("âš ï¸  æ— æ³•æž„å»ºè¶‹åŠ¿æŸ¥è¯¢ï¼Œè·³è¿‡")
	}
}

// DiagnoseControllerManagerMetrics è¯Šæ–­Controller ManageræŒ‡æ ‡ï¼ˆå¢žå¼ºç‰ˆï¼‰
func (c *ClusterOperator) DiagnoseControllerManagerMetrics() map[string]interface{} {
	diagnosis := make(map[string]interface{})

	c.log.Infof("ðŸ” å¼€å§‹å®Œæ•´è¯Šæ–­ Controller Manager æŒ‡æ ‡...")

	// 1. è¯Šæ–­workqueueæŒ‡æ ‡
	info := c.diagnoseWorkqueueMetrics()
	diagnosis["workqueue_available"] = info.available
	diagnosis["workqueue_metric_name"] = info.metricName
	diagnosis["workqueue_count"] = len(info.nameMap)
	diagnosis["workqueue_names"] = info.nameMap
	diagnosis["matched_queries"] = info.matchedQueries

	// 2. æ£€æŸ¥Controller Managerè¿›ç¨‹çŠ¶æ€
	cmUpQuery := `up{job=~".*controller.*manager.*"}`
	cmResults, err := c.query(cmUpQuery, nil)
	diagnosis["controller_manager_up_query_success"] = err == nil
	diagnosis["controller_manager_instances"] = len(cmResults)

	if len(cmResults) > 0 {
		upCount := 0
		for _, r := range cmResults {
			if r.Value > 0 {
				upCount++
			}
		}
		diagnosis["controller_manager_up_count"] = upCount
	}

	// 3. æ£€æŸ¥leaderçŠ¶æ€
	leaderQueries := []string{
		`leader_election_master_status{name=~".*controller.*manager.*"}`,
		`leader_election_master_status{name="kube-controller-manager"}`,
		`leader_election_master_status`,
	}

	for i, query := range leaderQueries {
		results, err := c.query(query, nil)
		if err == nil && len(results) > 0 {
			diagnosis["leader_query_success"] = true
			diagnosis["leader_query_index"] = i
			diagnosis["leader_instances"] = len(results)

			for _, r := range results {
				if r.Value > 0 {
					diagnosis["has_leader"] = true
					diagnosis["leader_metric"] = r.Metric
					break
				}
			}
			break
		}
	}

	// 4. æ£€æŸ¥å„ç§workqueueæŒ‡æ ‡çš„å¯ç”¨æ€§
	metricChecks := map[string]string{
		"workqueue_depth":                     `workqueue_depth`,
		"workqueue_adds_total":                `workqueue_adds_total`,
		"workqueue_retries_total":             `workqueue_retries_total`,
		"workqueue_queue_duration_seconds":    `workqueue_queue_duration_seconds_bucket`,
		"workqueue_work_duration_seconds":     `workqueue_work_duration_seconds_bucket`,
		"workqueue_longest_running_processor": `workqueue_longest_running_processor_seconds`,
		"controller_runtime_reconcile_errors": `controller_runtime_reconcile_errors_total`,
		"controller_runtime_reconcile_time":   `controller_runtime_reconcile_time_seconds_bucket`,
	}

	availableMetrics := make(map[string]bool)
	for name, query := range metricChecks {
		results, err := c.query(query, nil)
		availableMetrics[name] = err == nil && len(results) > 0
	}
	diagnosis["available_metrics"] = availableMetrics

	// 5. ç»Ÿè®¡å»ºè®®
	suggestions := []string{}
	if !info.available {
		suggestions = append(suggestions, "Controller Manager æœªæš´éœ² workqueue_depth æŒ‡æ ‡ï¼Œè¯·æ£€æŸ¥ --bind-address é…ç½®")
	}
	if !diagnosis["has_leader"].(bool) {
		suggestions = append(suggestions, "æœªæ£€æµ‹åˆ° Leaderï¼Œè¯·æ£€æŸ¥ leader election é…ç½®")
	}
	if len(info.matchedQueries) < 4 {
		suggestions = append(suggestions, fmt.Sprintf("åªåŒ¹é…åˆ° %d ä¸ª controller é˜Ÿåˆ—ï¼Œå¯èƒ½æœ‰é—æ¼", len(info.matchedQueries)))
	}
	diagnosis["suggestions"] = suggestions

	c.log.Infof("âœ… è¯Šæ–­å®Œæˆï¼Œå»ºè®®: %v", suggestions)
	return diagnosis
}

// ==================== è¾…åŠ©å‡½æ•° ====================

// discoverWorkqueues å‘çŽ°æ‰€æœ‰å¯ç”¨çš„workqueue
func (c *ClusterOperator) discoverWorkqueues() (map[string]bool, error) {
	allWorkqueueQuery := `workqueue_depth`
	results, err := c.query(allWorkqueueQuery, nil)
	if err != nil {
		return nil, err
	}

	workqueueMap := make(map[string]bool)
	for _, result := range results {
		if name, ok := result.Metric["name"]; ok && name != "" {
			workqueueMap[name] = true
		}
	}

	return workqueueMap, nil
}

// buildWorkqueueQuery æž„å»ºworkqueueæŸ¥è¯¢ï¼ˆæ”¹è¿›ç‰ˆï¼‰
func (c *ClusterOperator) buildWorkqueueQuery(workqueueMap map[string]bool, keywords []string) string {
	if len(workqueueMap) == 0 {
		// é™çº§ï¼šä½¿ç”¨æ­£åˆ™åŒ¹é…
		pattern := strings.Join(keywords, "|")
		return fmt.Sprintf(`sum(workqueue_depth{name=~"(?i).*(%s).*"})`, pattern)
	}

	// ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
	for _, keyword := range keywords {
		for name := range workqueueMap {
			if strings.EqualFold(name, keyword) {
				c.log.Infof("âœ… ç²¾ç¡®åŒ¹é… workqueue: %s", name)
				return fmt.Sprintf(`sum(workqueue_depth{name="%s"})`, name)
			}
		}
	}

	// ç­–ç•¥2ï¼šåŒ…å«åŒ¹é…ï¼ˆä¼˜å…ˆåŒ¹é…æœ€çŸ­çš„åç§°ï¼‰
	var bestMatch string
	minLen := 999
	for name := range workqueueMap {
		nameLower := strings.ToLower(name)
		for _, keyword := range keywords {
			keywordLower := strings.ToLower(keyword)
			if strings.Contains(nameLower, keywordLower) {
				if len(name) < minLen {
					bestMatch = name
					minLen = len(name)
				}
			}
		}
	}
	if bestMatch != "" {
		c.log.Infof("âœ… åŒ…å«åŒ¹é… workqueue: %s", bestMatch)
		return fmt.Sprintf(`sum(workqueue_depth{name="%s"})`, bestMatch)
	}

	// ç­–ç•¥3ï¼šåŽç¼€åŒ¹é…ï¼ˆcontrolleråŽç¼€ï¼‰
	for _, keyword := range keywords {
		suffix := keyword + "_controller"
		for name := range workqueueMap {
			if strings.HasSuffix(strings.ToLower(name), strings.ToLower(suffix)) {
				c.log.Infof("âœ… åŽç¼€åŒ¹é… workqueue: %s", name)
				return fmt.Sprintf(`sum(workqueue_depth{name="%s"})`, name)
			}
		}
	}

	// ç­–ç•¥4ï¼šæ­£åˆ™åŒ¹é…ï¼ˆæœ€åŽæ‰‹æ®µï¼‰
	pattern := strings.Join(keywords, "|")
	query := fmt.Sprintf(`sum(workqueue_depth{name=~"(?i).*(%s).*"})`, pattern)
	c.log.Errorf("âš ï¸  ä½¿ç”¨æ­£åˆ™åŒ¹é…: %s", query)
	return query
}

// queryControllerManagerTrend æŸ¥è¯¢è¶‹åŠ¿æ•°æ®
func (c *ClusterOperator) queryControllerManagerTrend(cm *types.ControllerManagerMetrics, timeRange *types.TimeRange, window string, workqueueMap map[string]bool) {
	step := timeRange.Step
	if step == "" {
		step = c.calculateStep(timeRange.Start, timeRange.End)
	}

	c.log.Infof("ðŸ“Š æŸ¥è¯¢ Controller Manager è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
		timeRange.Start.Format("2006-01-02 15:04:05"),
		timeRange.End.Format("2006-01-02 15:04:05"),
		step)

	var mu sync.Mutex
	rangeTasks := []clusterRangeTask{
		{
			name:  "deployment_queue_trend",
			query: c.buildWorkqueueQuery(workqueueMap, []string{"deployment"}),
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					cm.Trend = make([]types.ControllerManagerDataPoint, len(results[0].Values))
					for i, v := range results[0].Values {
						cm.Trend[i] = types.ControllerManagerDataPoint{
							Timestamp:            v.Timestamp,
							DeploymentQueueDepth: int64(v.Value),
						}
					}
				}
				return nil
			},
		},
		{
			name:  "total_queue_trend",
			query: `sum(workqueue_depth)`,
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					for i, v := range results[0].Values {
						if i < len(cm.Trend) {
							cm.Trend[i].TotalQueueDepth = int64(v.Value)
						}
					}
				}
				return nil
			},
		},
		{
			name:  "retry_rate_trend",
			query: fmt.Sprintf(`sum(rate(workqueue_retries_total[%s]))`, window),
			f: func(results []types.RangeQueryResult) error {
				if len(results) > 0 && len(results[0].Values) > 0 {
					mu.Lock()
					defer mu.Unlock()
					for i, v := range results[0].Values {
						if i < len(cm.Trend) {
							cm.Trend[i].RetryRate = v.Value
						}
					}
				}
				return nil
			},
		},
	}

	c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
	c.log.Infof("ðŸ“ˆ Controller Manager è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(cm.Trend))
}

// ==================== Etcd ä¼˜åŒ– ====================

// GetEtcdMetrics èŽ·å– Etcd æŒ‡æ ‡
func (c *ClusterOperator) GetEtcdMetrics(timeRange *types.TimeRange) (*types.EtcdMetrics, error) {
	etcd := &types.EtcdMetrics{Trend: []types.EtcdDataPoint{}}
	window := c.calculateRateWindow(timeRange)

	c.log.Infof("ðŸ“Š å¼€å§‹æŸ¥è¯¢ Etcd æŒ‡æ ‡ (window: %s)", window)

	// ==================== é›†ç¾¤çŠ¶æ€ ====================
	tasks := []clusterQueryTask{
		{
			name:  "has_leader",
			query: `etcd_server_has_leader`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.HasLeader = results[0].Value > 0
					c.log.Infof("âœ… Etcd HasLeader: %v", etcd.HasLeader)
				} else {
					c.log.Errorf("âš ï¸  etcd_server_has_leader æŒ‡æ ‡ä¸å­˜åœ¨")
				}
				return nil
			},
		},
		{
			name:  "leader_changes",
			query: `sum(increase(etcd_server_leader_changes_seen_total[1h]))`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.LeaderChanges = int64(results[0].Value)
					c.log.Infof("âœ… Etcd Leader Changes (1h): %d", etcd.LeaderChanges)
				}
				return nil
			},
		},
		{
			name:  "member_count",
			query: `count(etcd_server_has_leader)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.MemberCount = int64(results[0].Value)
					c.log.Infof("âœ… Etcd Member Count: %d", etcd.MemberCount)
				}
				return nil
			},
		},
		{
			name:  "is_learner",
			query: `sum(etcd_server_is_learner)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.IsLearner = results[0].Value > 0
				}
				return nil
			},
		},

		// ==================== å­˜å‚¨æŒ‡æ ‡ ====================
		{
			name:  "db_size",
			query: `sum(etcd_mvcc_db_total_size_in_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.DBSizeBytes = int64(results[0].Value)
					c.log.Infof("âœ… Etcd DB Size: %d bytes (%.2f MB)",
						etcd.DBSizeBytes, float64(etcd.DBSizeBytes)/1024/1024)
				}
				return nil
			},
		},
		{
			name:  "db_size_in_use",
			query: `sum(etcd_mvcc_db_total_size_in_use_in_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.DBSizeInUse = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "db_size_limit",
			query: `sum(etcd_server_quota_backend_bytes)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.DBSizeLimit = int64(results[0].Value)
					c.log.Infof("âœ… Etcd DB Size Limit: %d bytes (%.2f GB)",
						etcd.DBSizeLimit, float64(etcd.DBSizeLimit)/1024/1024/1024)
				}
				return nil
			},
		},
		{
			name:  "key_total",
			query: `sum(etcd_debugging_mvcc_keys_total)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.KeyTotal = int64(results[0].Value)
					c.log.Infof("âœ… Etcd Key Total: %d", etcd.KeyTotal)
				}
				return nil
			},
		},

		// ==================== æ€§èƒ½æŒ‡æ ‡ ====================
		{
			name:  "commit_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.CommitLatency = results[0].Value
					c.log.Infof("âœ… Etcd Commit Latency P99: %.4f s (%.2f ms)",
						etcd.CommitLatency, etcd.CommitLatency*1000)
				}
				return nil
			},
		},
		{
			name:  "wal_fsync",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.WALFsyncLatency = results[0].Value
					c.log.Infof("âœ… Etcd WAL Fsync Latency P99: %.4f s (%.2f ms)",
						etcd.WALFsyncLatency, etcd.WALFsyncLatency*1000)
				}
				return nil
			},
		},
		{
			name:  "apply_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_server_apply_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.ApplyLatency = results[0].Value
					c.log.Infof("âœ… Etcd Apply Latency P99: %.4f s", etcd.ApplyLatency)
				}
				return nil
			},
		},
		{
			name:  "snapshot_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_debugging_snap_save_total_duration_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.SnapshotLatency = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "compact_latency",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_bucket[%s])) by (le)) / 1000`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.CompactLatency = results[0].Value
				}
				return nil
			},
		},

		// ==================== ç½‘ç»œæŒ‡æ ‡ ====================
		{
			name:  "peer_rtt",
			query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_network_peer_round_trip_time_seconds_bucket[%s])) by (le))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.PeerRTT = results[0].Value
					c.log.Infof("âœ… Etcd Peer RTT P99: %.4f s (%.2f ms)",
						etcd.PeerRTT, etcd.PeerRTT*1000)
				}
				return nil
			},
		},
		{
			name:  "network_send",
			query: fmt.Sprintf(`sum(rate(etcd_network_peer_sent_bytes_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.NetworkSendRate = results[0].Value
					c.log.Infof("âœ… Etcd Network Send Rate: %.2f bytes/s", etcd.NetworkSendRate)
				}
				return nil
			},
		},
		{
			name:  "network_recv",
			query: fmt.Sprintf(`sum(rate(etcd_network_peer_received_bytes_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.NetworkRecvRate = results[0].Value
					c.log.Infof("âœ… Etcd Network Recv Rate: %.2f bytes/s", etcd.NetworkRecvRate)
				}
				return nil
			},
		},

		// ==================== æ“ä½œç»Ÿè®¡ ====================
		{
			name:  "proposals_failed",
			query: fmt.Sprintf(`sum(increase(etcd_server_proposals_failed_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.ProposalsFailed = int64(results[0].Value)
					c.log.Infof("âœ… Etcd Proposals Failed: %d", etcd.ProposalsFailed)
				}
				return nil
			},
		},
		{
			name:  "proposals_pending",
			query: `sum(etcd_server_proposals_pending)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.ProposalsPending = int64(results[0].Value)
					c.log.Infof("âœ… Etcd Proposals Pending: %d", etcd.ProposalsPending)
				}
				return nil
			},
		},
		{
			name:  "proposals_applied",
			query: fmt.Sprintf(`sum(increase(etcd_server_proposals_applied_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.ProposalsApplied = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "proposals_committed",
			query: fmt.Sprintf(`sum(increase(etcd_server_proposals_committed_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.ProposalsCommitted = int64(results[0].Value)
				}
				return nil
			},
		},

		// ==================== æ“ä½œé€ŸçŽ‡ ====================
		{
			name:  "get_rate",
			query: fmt.Sprintf(`sum(rate(etcd_mvcc_range_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.GetRate = results[0].Value
					c.log.Infof("âœ… Etcd Get Rate: %.2f ops/s", etcd.GetRate)
				}
				return nil
			},
		},
		{
			name:  "put_rate",
			query: fmt.Sprintf(`sum(rate(etcd_mvcc_put_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.PutRate = results[0].Value
					c.log.Infof("âœ… Etcd Put Rate: %.2f ops/s", etcd.PutRate)
				}
				return nil
			},
		},
		{
			name:  "delete_rate",
			query: fmt.Sprintf(`sum(rate(etcd_mvcc_delete_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.DeleteRate = results[0].Value
					c.log.Infof("âœ… Etcd Delete Rate: %.2f ops/s", etcd.DeleteRate)
				}
				return nil
			},
		},

		// ==================== æ…¢æ“ä½œ ====================
		{
			name:  "slow_applies",
			query: fmt.Sprintf(`sum(increase(etcd_server_slow_apply_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					etcd.SlowApplies = int64(results[0].Value)
					if etcd.SlowApplies > 0 {
						c.log.Errorf("âš ï¸  Etcd Slow Applies: %d", etcd.SlowApplies)
					}
				}
				return nil
			},
		},
		{
			name:  "slow_commits",
			query: fmt.Sprintf(`sum(increase(etcd_disk_backend_commit_duration_seconds_count{le="0.1"}[%s])) - sum(increase(etcd_disk_backend_commit_duration_seconds_count{le="0.025"}[%s]))`, window, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 && results[0].Value > 0 {
					etcd.SlowCommits = int64(results[0].Value)
					if etcd.SlowCommits > 0 {
						c.log.Errorf("âš ï¸  Etcd Slow Commits: %d", etcd.SlowCommits)
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// ==================== è¶‹åŠ¿æ•°æ® ====================
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		c.log.Infof("ðŸ“Š æŸ¥è¯¢ Etcd è¶‹åŠ¿æ•°æ®: start=%s, end=%s, step=%s",
			timeRange.Start.Format("2006-01-02 15:04:05"),
			timeRange.End.Format("2006-01-02 15:04:05"),
			step)

		var mu sync.Mutex
		rangeTasks := []clusterRangeTask{
			{
				name:  "db_size_trend",
				query: `sum(etcd_mvcc_db_total_size_in_bytes)`,
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						etcd.Trend = make([]types.EtcdDataPoint, len(results[0].Values))
						for i, v := range results[0].Values {
							etcd.Trend[i] = types.EtcdDataPoint{
								Timestamp:   v.Timestamp,
								DBSizeBytes: int64(v.Value),
							}
						}
					}
					return nil
				},
			},
			{
				name:  "commit_latency_trend",
				query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[%s])) by (le))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(etcd.Trend) {
								etcd.Trend[i].CommitLatency = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "proposals_failed_trend",
				query: fmt.Sprintf(`sum(increase(etcd_server_proposals_failed_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(etcd.Trend) {
								etcd.Trend[i].ProposalsFailed = int64(v.Value)
							}
						}
					}
					return nil
				},
			},
			{
				name:  "peer_rtt_trend",
				query: fmt.Sprintf(`histogram_quantile(0.99, sum(rate(etcd_network_peer_round_trip_time_seconds_bucket[%s])) by (le))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(etcd.Trend) {
								etcd.Trend[i].PeerRTT = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "get_rate_trend",
				query: fmt.Sprintf(`sum(rate(etcd_mvcc_range_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(etcd.Trend) {
								etcd.Trend[i].GetRate = v.Value
							}
						}
					}
					return nil
				},
			},
			{
				name:  "put_rate_trend",
				query: fmt.Sprintf(`sum(rate(etcd_mvcc_put_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 && len(results[0].Values) > 0 {
						mu.Lock()
						defer mu.Unlock()
						for i, v := range results[0].Values {
							if i < len(etcd.Trend) {
								etcd.Trend[i].PutRate = v.Value
							}
						}
					}
					return nil
				},
			},
		}

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)
		c.log.Infof("ðŸ“ˆ Etcd è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(etcd.Trend))
	}

	return etcd, nil
}

// ==================== å·¥ä½œè´Ÿè½½æŸ¥è¯¢ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================

func (c *ClusterOperator) GetClusterWorkloads(timeRange *types.TimeRange) (*types.ClusterWorkloadMetrics, error) {
	workloads := &types.ClusterWorkloadMetrics{}

	var wg sync.WaitGroup
	wg.Add(2)

	go func() {
		defer wg.Done()
		if pods, err := c.GetClusterPods(timeRange); err == nil {
			workloads.Pods = *pods
		}
	}()

	go func() {
		defer wg.Done()
		if deployments, err := c.GetClusterDeployments(); err == nil {
			workloads.Deployments = *deployments
		}
	}()

	wg.Wait()

	// å…¶ä»–å·¥ä½œè´Ÿè½½ç»Ÿè®¡
	workloads.StatefulSets = c.getClusterStatefulSets()
	workloads.DaemonSets = c.getClusterDaemonSets()
	workloads.Jobs = c.getClusterJobs()
	workloads.Services = c.getClusterServices()
	workloads.Containers = c.getClusterContainers()

	return workloads, nil
}

func (c *ClusterOperator) GetClusterPods(timeRange *types.TimeRange) (*types.ClusterPodMetrics, error) {
	pods := &types.ClusterPodMetrics{
		HighRestartPods: []types.PodRestartInfo{},
		Trend:           []types.ClusterPodDataPoint{},
	}

	//window := c.calculateRateWindow(timeRange)

	// åˆå¹¶æŸ¥è¯¢æ‰€æœ‰ Pod ç»Ÿè®¡
	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `sum(kube_pod_status_phase)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "running",
			query: `sum(kube_pod_status_phase{phase="Running"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Running = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pending",
			query: `sum(kube_pod_status_phase{phase="Pending"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Pending = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "failed",
			query: `sum(kube_pod_status_phase{phase="Failed"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Failed = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "succeeded",
			query: `sum(kube_pod_status_phase{phase="Succeeded"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Succeeded = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "unknown",
			query: `sum(kube_pod_status_phase{phase="Unknown"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Unknown = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "ready",
			query: `sum(kube_pod_status_ready{condition="true"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.Ready = int64(results[0].Value)
					pods.NotReady = pods.Total - pods.Ready
				}
				return nil
			},
		},
		{
			name:  "restarts",
			query: `sum(kube_pod_container_status_restarts_total)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					pods.TotalRestarts = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "high_restart",
			query: `topk(10, sum by (namespace, pod) (kube_pod_container_status_restarts_total))`,
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					namespace, okNs := r.Metric["namespace"]
					pod, okPod := r.Metric["pod"]
					if !okNs || !okPod {
						continue
					}
					restartCount := int64(r.Value)
					restartRate := 0.0
					if timeRange != nil && !timeRange.Start.IsZero() {
						duration := time.Since(timeRange.Start)
						hours := duration.Hours()
						if hours > 0 {
							restartRate = float64(restartCount) / hours
						}
					}
					pods.HighRestartPods = append(pods.HighRestartPods, types.PodRestartInfo{
						Namespace:    namespace,
						PodName:      pod,
						RestartCount: restartCount,
						RestartRate:  restartRate,
					})
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// è¶‹åŠ¿æ•°æ®
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		trendQuery := `sum(kube_pod_status_phase{phase="Running"})`
		if trendResult, err := c.queryRange(trendQuery, timeRange.Start, timeRange.End, step); err == nil && len(trendResult) > 0 {
			pods.Trend = c.parsePodTrend(trendResult[0].Values)
		}
	}

	return pods, nil
}

func (c *ClusterOperator) GetClusterDeployments() (*types.ClusterDeploymentMetrics, error) {
	deployments := &types.ClusterDeploymentMetrics{}

	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_deployment_created)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					deployments.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "available",
			query: `sum(kube_deployment_status_replicas_available)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					deployments.AvailableReplicas = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "unavailable",
			query: `sum(kube_deployment_status_replicas_unavailable)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					deployments.UnavailableReplicas = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "updating",
			query: `count(kube_deployment_status_replicas_updated != kube_deployment_spec_replicas)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					deployments.Updating = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return deployments, nil
}

func (c *ClusterOperator) GetClusterNetwork(timeRange *types.TimeRange) (*types.ClusterNetworkMetrics, error) {
	network := &types.ClusterNetworkMetrics{Trend: []types.ClusterNetworkDataPoint{}}
	window := c.calculateRateWindow(timeRange)

	// ==================== 1. æŸ¥è¯¢å½“å‰æµé‡é€ŸçŽ‡ ====================
	tasks := []clusterQueryTask{
		{
			name:  "ingress",
			query: fmt.Sprintf(`sum(rate(container_network_receive_bytes_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					network.TotalIngressBytesPerSec = results[0].Value
				}
				return nil
			},
		},
		{
			name:  "egress",
			query: fmt.Sprintf(`sum(rate(container_network_transmit_bytes_total[%s]))`, window),
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					network.TotalEgressBytesPerSec = results[0].Value
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// ==================== 2. æŸ¥è¯¢è¶‹åŠ¿æ•°æ® ====================
	if timeRange != nil && !timeRange.Start.IsZero() && !timeRange.End.IsZero() {
		step := timeRange.Step
		if step == "" {
			step = c.calculateStep(timeRange.Start, timeRange.End)
		}

		c.log.Infof("ðŸ“Š æŸ¥è¯¢ç½‘ç»œæµé‡è¶‹åŠ¿: start=%s, end=%s, step=%s, window=%s",
			timeRange.Start.Format(time.RFC3339),
			timeRange.End.Format(time.RFC3339),
			step,
			window)

		var mu sync.Mutex
		rangeTasks := []clusterRangeTask{
			{
				name:  "ingress_trend",
				query: fmt.Sprintf(`sum(rate(container_network_receive_bytes_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()

						c.log.Infof("âœ… å…¥æµé‡è¶‹åŠ¿æ•°æ®ç‚¹æ•°: %d", len(results[0].Values))

						// åˆå§‹åŒ– Trend æ•°ç»„
						network.Trend = make([]types.ClusterNetworkDataPoint, len(results[0].Values))
						for i, v := range results[0].Values {
							network.Trend[i] = types.ClusterNetworkDataPoint{
								Timestamp:          v.Timestamp,
								IngressBytesPerSec: v.Value,
							}
						}
					} else {
						c.log.Errorf("âš ï¸  å…¥æµé‡è¶‹åŠ¿æ•°æ®ä¸ºç©º")
					}
					return nil
				},
			},
			{
				name:  "egress_trend",
				query: fmt.Sprintf(`sum(rate(container_network_transmit_bytes_total[%s]))`, window),
				f: func(results []types.RangeQueryResult) error {
					if len(results) > 0 {
						mu.Lock()
						defer mu.Unlock()

						c.log.Infof("âœ… å‡ºæµé‡è¶‹åŠ¿æ•°æ®ç‚¹æ•°: %d", len(results[0].Values))

						// å°†å‡ºæµé‡æ•°æ®å¡«å……åˆ°å·²æœ‰çš„ Trend æ•°ç»„ä¸­
						for i, v := range results[0].Values {
							if i < len(network.Trend) {
								network.Trend[i].EgressBytesPerSec = v.Value
							}
						}
					} else {
						c.log.Errorf("âš ï¸  å‡ºæµé‡è¶‹åŠ¿æ•°æ®ä¸ºç©º")
					}
					return nil
				},
			},
		}

		c.executeParallelRangeQueries(timeRange.Start, timeRange.End, step, rangeTasks)

		c.log.Infof("ðŸ“ˆ ç½‘ç»œæµé‡è¶‹åŠ¿æ•°æ®æŸ¥è¯¢å®Œæˆï¼Œå…± %d ä¸ªæ•°æ®ç‚¹", len(network.Trend))
	} else {
		c.log.Errorf("âš ï¸  æœªæä¾›æ—¶é—´èŒƒå›´ï¼Œè·³è¿‡è¶‹åŠ¿æ•°æ®æŸ¥è¯¢")
	}

	return network, nil
}

func (c *ClusterOperator) GetClusterStorage() (*types.ClusterStorageMetrics, error) {
	storage := &types.ClusterStorageMetrics{StorageClasses: []types.StorageClassUsage{}}

	// âœ… ä¿®å¤ï¼šä½¿ç”¨ sum() ç»Ÿè®¡çŠ¶æ€æŒ‡æ ‡
	tasks := []clusterQueryTask{
		{
			name: "pv_total",
			// âœ… æ–¹æ¡ˆ1ï¼šä½¿ç”¨ sum ç»Ÿè®¡æ‰€æœ‰çŠ¶æ€
			query: `sum(kube_persistentvolume_status_phase)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVTotal = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name: "pv_bound",
			// âœ… ä½¿ç”¨ sum è€Œä¸æ˜¯ count
			query: `sum(kube_persistentvolume_status_phase{phase="Bound"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVBound = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pv_available",
			query: `sum(kube_persistentvolume_status_phase{phase="Available"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVAvailable = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pv_released",
			query: `sum(kube_persistentvolume_status_phase{phase="Released"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVReleased = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pv_failed",
			query: `sum(kube_persistentvolume_status_phase{phase="Failed"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVFailed = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name: "pvc_total",
			// âœ… ä½¿ç”¨ sum ç»Ÿè®¡æ‰€æœ‰çŠ¶æ€
			query: `sum(kube_persistentvolumeclaim_status_phase)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVCTotal = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pvc_bound",
			query: `sum(kube_persistentvolumeclaim_status_phase{phase="Bound"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVCBound = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pvc_pending",
			query: `sum(kube_persistentvolumeclaim_status_phase{phase="Pending"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVCPending = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "pvc_lost",
			query: `sum(kube_persistentvolumeclaim_status_phase{phase="Lost"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					storage.PVCLost = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)

	// StorageClass ç»Ÿè®¡ï¼ˆä¿æŒä¸å˜ï¼‰
	scBytesQuery := `sum by (storageclass) (kube_persistentvolumeclaim_resource_requests_storage_bytes)`
	scCountQuery := `count by (storageclass) (kube_persistentvolumeclaim_info)`

	var wg sync.WaitGroup
	wg.Add(2)

	var scBytesResults, scCountResults []types.InstantQueryResult

	go func() {
		defer wg.Done()
		scBytesResults, _ = c.query(scBytesQuery, nil)
	}()

	go func() {
		defer wg.Done()
		scCountResults, _ = c.query(scCountQuery, nil)
	}()

	wg.Wait()

	// åˆå¹¶ StorageClass ç»“æžœ
	scCountMap := make(map[string]int64)
	for _, r := range scCountResults {
		if sc, ok := r.Metric["storageclass"]; ok {
			scCountMap[sc] = int64(r.Value)
		}
	}

	for _, r := range scBytesResults {
		if sc, ok := r.Metric["storageclass"]; ok {
			storage.StorageClasses = append(storage.StorageClasses, types.StorageClassUsage{
				StorageClass: sc,
				PVCCount:     scCountMap[sc],
				TotalBytes:   int64(r.Value),
			})
		}
	}

	return storage, nil
}
func (c *ClusterOperator) GetClusterNamespaces(timeRange *types.TimeRange) (*types.ClusterNamespaceMetrics, error) {
	namespaces := &types.ClusterNamespaceMetrics{
		TopByCPU:    []types.NamespaceResourceRanking{},
		TopByMemory: []types.NamespaceResourceRanking{},
		TopByPods:   []types.NamespaceResourceRanking{},
	}

	window := c.calculateRateWindow(timeRange)

	// åˆå¹¶æŸ¥è¯¢
	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_namespace_created)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					namespaces.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "active",
			query: `count(kube_namespace_status_phase{phase="Active"})`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					namespaces.Active = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "top_cpu",
			query: fmt.Sprintf(`topk(10, sum by (namespace) (rate(container_cpu_usage_seconds_total{container!=""}[%s])))`, window),
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if namespace, ok := r.Metric["namespace"]; ok {
						namespaces.TopByCPU = append(namespaces.TopByCPU, types.NamespaceResourceRanking{
							Namespace: namespace,
							Value:     r.Value,
							Unit:      "cores",
						})
					}
				}
				return nil
			},
		},
		{
			name:  "top_memory",
			query: `topk(10, sum by (namespace) (container_memory_working_set_bytes{container!=""}))`,
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if namespace, ok := r.Metric["namespace"]; ok {
						namespaces.TopByMemory = append(namespaces.TopByMemory, types.NamespaceResourceRanking{
							Namespace: namespace,
							Value:     r.Value,
							Unit:      "bytes",
						})
					}
				}
				return nil
			},
		},
		{
			name:  "top_pods",
			query: `topk(10, count by (namespace) (kube_pod_info))`,
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if namespace, ok := r.Metric["namespace"]; ok {
						namespaces.TopByPods = append(namespaces.TopByPods, types.NamespaceResourceRanking{
							Namespace: namespace,
							Value:     r.Value,
							Unit:      "pods",
						})
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return namespaces, nil
}

// ==================== è¾…åŠ©æ–¹æ³• ====================

func (c *ClusterOperator) getClusterStatefulSets() types.ClusterStatefulSetMetrics {
	sts := types.ClusterStatefulSetMetrics{}

	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_statefulset_created)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					sts.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "ready",
			query: `sum(kube_statefulset_status_replicas_ready)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					sts.ReadyReplicas = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "current",
			query: `sum(kube_statefulset_status_replicas_current)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					sts.CurrentReplicas = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return sts
}

func (c *ClusterOperator) getClusterDaemonSets() types.ClusterDaemonSetMetrics {
	ds := types.ClusterDaemonSetMetrics{}

	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_daemonset_created)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					ds.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "desired",
			query: `sum(kube_daemonset_status_desired_number_scheduled)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					ds.DesiredPods = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "available",
			query: `sum(kube_daemonset_status_number_available)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					ds.AvailablePods = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "unavailable",
			query: `sum(kube_daemonset_status_number_unavailable)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					ds.UnavailablePods = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return ds
}

func (c *ClusterOperator) getClusterJobs() types.ClusterJobMetrics {
	jobs := types.ClusterJobMetrics{}

	tasks := []clusterQueryTask{
		{
			name:  "active",
			query: `count(kube_job_status_active > 0)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					jobs.Active = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "succeeded",
			query: `count(kube_job_status_succeeded > 0)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					jobs.Succeeded = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "failed",
			query: `count(kube_job_status_failed > 0)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					jobs.Failed = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "cronjobs",
			query: `count(kube_cronjob_created)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					jobs.CronJobsTotal = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return jobs
}

func (c *ClusterOperator) getClusterServices() types.ClusterServiceMetrics {
	services := types.ClusterServiceMetrics{ByType: []types.ServiceTypeCount{}}

	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_service_info)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					services.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "by_type",
			query: `count by (type) (kube_service_info)`,
			f: func(results []types.InstantQueryResult) error {
				for _, r := range results {
					if serviceType, ok := r.Metric["type"]; ok {
						services.ByType = append(services.ByType, types.ServiceTypeCount{
							Type:  serviceType,
							Count: int64(r.Value),
						})
					}
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return services
}

func (c *ClusterOperator) getClusterContainers() types.ClusterContainerMetrics {
	containers := types.ClusterContainerMetrics{}

	tasks := []clusterQueryTask{
		{
			name:  "total",
			query: `count(kube_pod_container_info)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					containers.Total = int64(results[0].Value)
				}
				return nil
			},
		},
		{
			name:  "running",
			query: `count(kube_pod_container_status_running)`,
			f: func(results []types.InstantQueryResult) error {
				if len(results) > 0 {
					containers.Running = int64(results[0].Value)
				}
				return nil
			},
		},
	}

	c.executeParallelQueries(tasks)
	return containers
}

// ==================== æŸ¥è¯¢æ–¹æ³• ====================

func (c *ClusterOperator) query(query string, timestamp *time.Time) ([]types.InstantQueryResult, error) {
	params := map[string]string{"query": query}
	if timestamp != nil {
		params["time"] = c.formatTimestamp(*timestamp)
	}

	var response struct {
		Status string `json:"status"`
		Data   struct {
			Result []struct {
				Metric map[string]string `json:"metric"`
				Value  []interface{}     `json:"value"`
			} `json:"result"`
		} `json:"data"`
		Error string `json:"error,omitempty"`
	}

	if err := c.doRequest("GET", "/api/v1/query", params, nil, &response); err != nil {
		return nil, err
	}

	if response.Status != "success" {
		return nil, fmt.Errorf("æŸ¥è¯¢å¤±è´¥: %s", response.Error)
	}

	results := make([]types.InstantQueryResult, 0, len(response.Data.Result))
	for _, item := range response.Data.Result {
		if len(item.Value) != 2 {
			continue
		}
		timestampFloat, _ := item.Value[0].(float64)
		valueStr, _ := item.Value[1].(string)
		value, _ := strconv.ParseFloat(valueStr, 64)

		results = append(results, types.InstantQueryResult{
			Metric: item.Metric,
			Value:  value,
			Time:   time.Unix(int64(timestampFloat), 0),
		})
	}

	return results, nil
}

func (c *ClusterOperator) queryRange(query string, start, end time.Time, step string) ([]types.RangeQueryResult, error) {
	params := map[string]string{
		"query": query,
		"start": c.formatTimestamp(start),
		"end":   c.formatTimestamp(end),
		"step":  step,
	}

	var response struct {
		Status string `json:"status"`
		Data   struct {
			Result []struct {
				Metric map[string]string `json:"metric"`
				Values [][]interface{}   `json:"values"`
			} `json:"result"`
		} `json:"data"`
		Error string `json:"error,omitempty"`
	}

	if err := c.doRequest("GET", "/api/v1/query_range", params, nil, &response); err != nil {
		return nil, err
	}

	if response.Status != "success" {
		return nil, fmt.Errorf("æŸ¥è¯¢å¤±è´¥: %s", response.Error)
	}

	results := make([]types.RangeQueryResult, 0, len(response.Data.Result))
	for _, item := range response.Data.Result {
		values := make([]types.MetricValue, 0, len(item.Values))
		for _, v := range item.Values {
			if len(v) != 2 {
				continue
			}
			timestampFloat, _ := v[0].(float64)
			valueStr, _ := v[1].(string)
			value, _ := strconv.ParseFloat(valueStr, 64)

			values = append(values, types.MetricValue{
				Timestamp: time.Unix(int64(timestampFloat), 0),
				Value:     value,
			})
		}

		if len(values) > 0 {
			results = append(results, types.RangeQueryResult{
				Metric: item.Metric,
				Values: values,
			})
		}
	}

	return results, nil
}

func (c *ClusterOperator) formatTimestamp(t time.Time) string {
	return fmt.Sprintf("%.3f", float64(t.UnixNano())/1e9)
}

func (c *ClusterOperator) addStatusCode(dist *types.StatusCodeDistribution, code string, value float64) {
	if len(code) < 1 {
		return
	}
	switch code[0] {
	case '2':
		dist.Status2xx[code] = value
	case '3':
		dist.Status3xx[code] = value
	case '4':
		dist.Status4xx[code] = value
	case '5':
		dist.Status5xx[code] = value
	}
}

func (c *ClusterOperator) parseAPIServerTrend(values []types.MetricValue) []types.APIServerDataPoint {
	trend := make([]types.APIServerDataPoint, 0, len(values))
	for _, v := range values {
		trend = append(trend, types.APIServerDataPoint{
			Timestamp:         v.Timestamp,
			RequestsPerSecond: v.Value,
		})
	}
	return trend
}

func (c *ClusterOperator) parseEtcdTrend(values []types.MetricValue) []types.EtcdDataPoint {
	trend := make([]types.EtcdDataPoint, 0, len(values))
	for _, v := range values {
		trend = append(trend, types.EtcdDataPoint{
			Timestamp:   v.Timestamp,
			DBSizeBytes: int64(v.Value),
		})
	}
	return trend
}

func (c *ClusterOperator) parseSchedulerTrend(values []types.MetricValue) []types.SchedulerDataPoint {
	trend := make([]types.SchedulerDataPoint, 0, len(values))
	for _, v := range values {
		trend = append(trend, types.SchedulerDataPoint{
			Timestamp:           v.Timestamp,
			ScheduleSuccessRate: v.Value,
		})
	}
	return trend
}

func (c *ClusterOperator) parseControllerManagerTrend(values []types.MetricValue) []types.ControllerManagerDataPoint {
	trend := make([]types.ControllerManagerDataPoint, 0, len(values))
	for _, v := range values {
		trend = append(trend, types.ControllerManagerDataPoint{
			Timestamp:            v.Timestamp,
			DeploymentQueueDepth: int64(v.Value),
		})
	}
	return trend
}

func (c *ClusterOperator) parsePodTrend(values []types.MetricValue) []types.ClusterPodDataPoint {
	trend := make([]types.ClusterPodDataPoint, 0, len(values))
	for _, v := range values {
		trend = append(trend, types.ClusterPodDataPoint{
			Timestamp: v.Timestamp,
			Running:   int64(v.Value),
		})
	}
	return trend
}
